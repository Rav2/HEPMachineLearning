{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.15/01\n"
     ]
    }
   ],
   "source": [
    "import read_tree as rt\n",
    "\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import read_tree as rt\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "\"\"\"\n",
    "ten nowy bedzie uproszczony\n",
    "__init__(nazwa_folderu,tryb)\n",
    "    inputs:\n",
    "            nazwa_folderu: np \"pierwszy_dataset\" tam bedzie pisac/stamtad bedzie szczytywac. \n",
    "                wydaje mi sie, ze musi byc to nazwa bez spacji oraz moze byc na przyklad \"folder/subfolder\"\n",
    "                To zapisuje nie tylko same dane ale także pomocnicze dane do wczytywania ich.\n",
    "            tryb: np 'r' lub 'w' i oznacza czy czytac ('r') czy pisac ('w')\n",
    "\n",
    "            \n",
    "write_general(features,l,przykladowy=False,co_ile_flush_file=10) \n",
    "        UWAGA to jest dostepne tylko w przypadku trybu 'w'\n",
    "        \n",
    "        features: to slwonik features dla jednego przykladu np {\"momentum\":[1.,0.,5.,7.]}\n",
    "            moze liczbe lub liste lub np array typu jakiegos int lub jakiegos float\n",
    "        l: to jest label jego 0 lub 1. Nie jestem pewien czy da się wpisywać po prostu\n",
    "            tensorflowowe tensory tutaj. na pewno można numpy obiekty. Prawdopodobnie także\n",
    "            listy pythonowe. \n",
    "\n",
    "        wygodna metoda do tego, azeby \"potasowac\" przyklady z roznych plikow w jeden dataset, bo pisze\n",
    "        sie przyklady w takiej formie w jakiej sie odczytywalo z tych datasetow. \n",
    "        \n",
    "        przykladowy: typu bool. Używamy niedomyślnej wartości True w przypadku, gdy utworzyliśmy obiekt\n",
    "            o trybie 'w' write i chcemy powiedzieć mu, jak wyglądać będa dane którymi zamierzamy go karmić. \n",
    "            czyli takiemu obiektowi wystarczy podać jeden taki przykładowy. Daje mu to wiedzę o tym, jak \n",
    "            dorabiać nowe featchers. To znaczy trzeba podać jeden przykładowy jeśli chcemy potem uzywać\n",
    "            .engineer_feature w tym trybie 'w'.\n",
    "        co_ile_flush_file to jest parametr oznaczający jak często mamy wyrzucać do pliku dane. \n",
    "        można poeksperymentować z jego wartością. nie wiem ile to ma wynosić. Może duża wartość \n",
    "        pozwoli szybciej zapisywać?\n",
    "\n",
    "write_from_tree(legs,jets,global_params,properties,l,co_ile_flush_file=10,przykladowy=False)\n",
    "        metoda w prosty sposob korzystajaca z funkcji metody write_general\n",
    "        dostepna tylko jak tryb to jest 'w'\n",
    "            legs,jets,global_params,properties: jak w wyjsciu klasy read_tree, tylko \"dla jednego przypadku\"\n",
    "                wiec jest tak\n",
    "                        zakladam, ze legs to jest lista o shapie (?,4) wypelniona floatami\n",
    "                        jets dokladnie tak samo\n",
    "                        global_params to jest w postaci {nazwa:liczba, ...}\n",
    "                        properties tak samo\n",
    "                        l to label jest intem rownym to 0 lub 1, gdzie 1 oznacza, ze to jest raczej bardziej ciekawy przypadek\n",
    "                         a 0 to taki bardziej tlo. to jest int \n",
    "                 co_ile_flush_file: to znaczy jak czesto ma oprozniac swoj buffer, liczy sie tylko\n",
    "                    gdy tryb=='w', nie wiem ile ma wynosic,wiec jak wiesz to smialo ustaw\n",
    "        \n",
    "                parametr przykladowy sluzy do tego, ze jak chcemy uzyc .engineer_feature() w trybie 'w' to \n",
    "        potrzeba podac przykladowe dane zeby obiekt wiedzial, czy dorabianie featcherow\n",
    "        jest poprawne. To bedzie ulatwiac uzytkownikowi zycie. \n",
    "    \n",
    "                    \n",
    "write_from_tree_general(self,legs_list,properties_list,l,co_ile_flush_file=10,przykladowy=False)\n",
    "        nieco bardziej ogolna wersja poprzedniej metody, gdzie \n",
    "        legs oraz jets jako obiekty tych samych typow zostaly wrzócone do tej samej\n",
    "        listy, tak samo global_params oraz properties. czyli zeby \n",
    "        wywolac to tak jak poprzednia metode to piszemy\n",
    "        obiekt.write_from_tree_general([legs,jets],[global_params,properties],l)\n",
    "        i to robi to co poprzednia metoda, ale jes to epsion bardziej ogolne. \n",
    "        znow jest zbudowane na bazie metody .write_general, wiec latwo zmienic. \n",
    "        \n",
    "                parametr przykladowy sluzy do tego, ze jak chcemy uzyc .engineer_feature() w trybie 'w' to \n",
    "        potrzeba podac przykladowe dane zeby obiekt wiedzial, czy dorabianie featcherow\n",
    "        jest poprawne. To bedzie ulatwiac uzytkownikowi zycie. \n",
    "\n",
    "    \n",
    "\n",
    "close()\n",
    "            dostepna tylko dla tryb=='w'\n",
    "            zamyka bezpiecznie nasz plik, tak by mozna go bylo uzyc przy czytaniu\n",
    "read()\n",
    "            dostepna tylko dla tryb=='r'\n",
    "            wyrzuci z siebie tensorflowowy dataset gotowy do uczenia\n",
    "                w przyszlosci Teraz dorabiam to, ze bedzie z siebire wyrzucac wraz z \n",
    "                nowymi dorobionymi featcherami\n",
    "            dataset wyglada tak, ze pojedynczy przypadek to jest (slownik_featurow,label)\n",
    "                gdzie label to bedzie 0 lub 1\n",
    "                zas slownik featerow to jest tak, ze sa klucze stringow a wartosci to jest\n",
    "                1d arraye ktore sa intami lub floatami \n",
    "                to znaczy, ze zwraca to samo co ta stara moja klasa czytajaca\n",
    "types()\n",
    "            \n",
    "            zwraca slownik typow naszego datasetu. to znaczy, ze zwraca slownik\n",
    "            {'nazwa_featchera':(4,'f')} jesli featcher o tej nazwie to lista 4 floatow\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "engineer_feature(self,f,slownik,typ,nazwa,naprawde_zapisz=True):\n",
    "        To mozna uzywac w trybie 'w' jak i w trybie 'r', ale znaczenie jest inne. \n",
    "        w trybie 'r' dziala tak, ze bedzie nam dorabiac featchers \n",
    "        tak 'on the fly' gdy uzyjemy\n",
    "        metody .read(). \n",
    "        Jezeli uzyjemy tej funkcji w trybie 'w' to naszym celem jest \n",
    "        dorobienie featcher w zapisanym binarnym pliku i zapisac nowe featchery. \n",
    "        Tutaj instrukcja jest taka, ze po inicjalizacji obiektu typu 'w' \n",
    "        uzywamy jednej z metod write costam, ale z parametrem 'przykladowy=True'. \n",
    "        Teraz mozemy dorabiac featchers przy pomocy tej metody. w trakcie tego \n",
    "        mozna sobie patrzeć przy pomocy metody .types() jak się nam zmieniają\n",
    "        typy featcherów jakie mamy. Nastepnie po zakończeniu tych czynnosći\n",
    "        zaczynamy uzywać normalnie metody .write costam bez parametru przykladowy. \n",
    "        \n",
    "        Chciałem zrobić coś takiego, że w trybie 'w' podaje się do wyboru czy \n",
    "        zapisywać powstały featcher na stałe w pliku z danymi czy jakoś zapamiętać że takie\n",
    "        coś ma być dorabiane w locie przy wczytywaniu, ale chyba nie da się \n",
    "        \n",
    "        \n",
    "       \n",
    "        \n",
    "        z powodzeniem uzywac wczesniej zrobionych features do produkcji jeszcze nowszych.\n",
    "        nazwa: czyli jak ten nowy ma sie nazywac\n",
    "        \n",
    "        f: to funkcja przyjmująca argumenty o nazwach ze zbioru kluczy slownika slownik, \n",
    "                zwraca zas nowy feature (czyli tensor o ksztalcie (-1,).\n",
    "                w trybie 'r'\n",
    "                musi to byc funkcja\n",
    "                dzialajaca dobrze na tensorach z tensorflow. To jest tak fajnie zaklepane, ze\n",
    "                wyrzuci blad jesli funkcja jest niepoprawna od razu przy wywolaniu tej \n",
    "                engineer_feature.\n",
    "                 funkcja f musi byc taka, ze dobrze dziala na tensorflowywch\n",
    "                 tensorach o ksztalcie (-1,) to znaczy \n",
    "                scisle jednowymiarowych. Wynikiem tej funkcji czyli nowym featurem \n",
    "                musi byc znowu tensor o ksztalcie\n",
    "                (-1,). \n",
    "                \n",
    "                w trybie 'w' ta funkcja 'f' ma dzialac nie na tensorflowowe tensory\n",
    "                a na takiego typu tensory, co sa w naszych danych ktorymi karmimy\n",
    "                w .write_general. \n",
    "                \n",
    "                w trybie 'r' jak podacie zla funkcje f to metoda engineer_feature od \n",
    "                razu wyrzuca blad, zas w trybie 'w' nie wyrzuca od razu bledu dopiero\n",
    "                przy zapisywani pojawi się jakis dziwny blad. \n",
    "        \n",
    "        \n",
    "                https://www.tensorflow.org/api_guides/python/math_ops\n",
    "                tu macie podstawowe operacje. pamietjcie, ze * oraz + tez mozna uzywac, ale\n",
    "                nie wszystkie funkcje z numpy sa dobre w tensorflow( to znaczy inaczej sie w nim nazywaja).\n",
    "\n",
    "        \n",
    "        slownik:  to slownik którego klucze sa ze zbioru nazw argumentow funkcji f zas \n",
    "            zas wartosci to sa nazwy rzeczy wystepujacych w kluczach slownika z metody .types()\n",
    "            i to mowi jakie nalezy rzeczy z dataset wstawic do funkcji f azeby otrzymac nowy feature\n",
    "        typ: wynosi np (4,'f') cyzli ze ten nowy\n",
    "            feature bedzie mial 4 floaty. moze byc tez 'i'. oznacza, \n",
    "            czy to co powstaje bedzie intem czy floatem. \n",
    "            Tak wiem to leniwe, ale bardziej bugoodporne po mojej stronie. \n",
    "        naprawde_zapisz: odnosi sie tylko do przypadku, gdy tryb to 'w' i oznacza odpowiedz na pytanie,\n",
    "            czy zapisac na dysku nasz nowy featcher czy tylko zapisac informacje o tym jak go odwtorzyc przy wczytywaniu. \n",
    "            wazne\n",
    "            najpierw podajecie te ktore maja byc zapisane na stale. potem podajecie te ktora\n",
    "            maja byc zapisane na niby to znaczy naprawde_zapisz=False. Jest to po to, azeby\n",
    "            nie bylo tak, ze przy czytaniu datasetu trzeba korzystac z zmiennych ktorych nie ma.\n",
    "            Dodatkowo jest tak, ze jak dodajemy featcher na niby to w nim ta funkcja f musi \n",
    "            czytać tensory typu tensorflow, zas jesli to jest featcher na prawde to \n",
    "            wowczas ta funkcja f ma miec tylo taka wlasnosc, ze dziala dobrze na \n",
    "            tensory wrzucane do metod write costam. \n",
    "            \n",
    "\n",
    "            \n",
    "       \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "class Na_niby_featcher(object):\n",
    "    def __init__(self, eng):\n",
    "        self.eng=eng\n",
    "    def to_dict(self):\n",
    "        return self.eng\n",
    "\n",
    "\n",
    "class Io_tf_binary_general:\n",
    "    def __init__(self,nazwa_folderu,tryb):\n",
    "        \n",
    "        self.nazwa_folderu=nazwa_folderu\n",
    "        def odczytaj_na_niby_featcheres():\n",
    "            with open(self.nazwa_folderu+'/on_the_fly_featcheres.pkl', 'rb') as input:\n",
    "                wyrzut=[]\n",
    "                rob=True\n",
    "                while rob:\n",
    "                    try:\n",
    "                        wyrzut.append(pickle.load(input))\n",
    "                    except:\n",
    "                        rob=False\n",
    "                return wyrzut\n",
    "        self.tryb=tryb\n",
    "        \n",
    "        if tryb=='r':\n",
    "            slownik_typow=Io_tf_binary_general.wczytaj_json(self.nazwa_folderu+\"/metadata\")\n",
    "            self.wewnetrzny=Io_tf_binary_general.Io_tf_binary_stary(\n",
    "                self.nazwa_folderu+\"/dane\",slownik_typow,self.tryb)\n",
    "            na_niby_featchers=odczytaj_na_niby_featcheres()\n",
    "            for eng in na_niby_featchers:\n",
    "                self.wewnetrzny.engineer_feature(**(eng.to_dict()))\n",
    "            \n",
    "        \n",
    "        self.nowopowstala=True\n",
    "        if self.tryb=='w':\n",
    "            self.new_featcheres_naprawde_zapisz=[]\n",
    "            self.new_featcheres_na_niby_zapisz=[]\n",
    "            self.pojawilo_sie_na_niby=False\n",
    "            self.typy_naprawde={}\n",
    "            self.typy_naniby={}\n",
    "            self.typy_pierwsze={}\n",
    "            self.juz_poznane=False\n",
    "        \n",
    "    def engineer_feature(self,f,slownik,typ,nazwa,naprawde_zapisz=True):\n",
    "        if self.tryb=='r':\n",
    "            assert not (nazwa in self.wewnetrzny.types().keys())\n",
    "            self.wewnetrzny.engineer_feature(f,slownik,typ,nazwa)\n",
    "        else:\n",
    "            assert not (nazwa in self.przyklad.keys())\n",
    "            podstawienia={}\n",
    "            for k in slownik.keys():\n",
    "                podstawienia[k]=self.przyklad[slownik[k]]\n",
    "            self.przyklad[nazwa]=f(**podstawienia)\n",
    "            #self.typy[nazwa]=typ\n",
    "            if naprawde_zapisz:\n",
    "                assert self.pojawilo_sie_na_niby==False\n",
    "                self.new_featcheres_naprawde_zapisz.append(\n",
    "            {'f':f,'slownik':slownik,'typ':typ,'nazwa':nazwa})\n",
    "                self.typy_naprawde[nazwa]=typ\n",
    "                self.typy_naniby[nazwa]=typ\n",
    "            else:\n",
    "                self.pojawilo_sie_na_niby=True\n",
    "                self.new_featcheres_na_niby_zapisz.append(\n",
    "            {'f':f,'slownik':slownik,'typ':typ,'nazwa':nazwa})\n",
    "                self.typy_naniby[nazwa]=typ\n",
    "            \n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "    \n",
    "    def zapisz_json(co,gdzie):\n",
    "        f=open(gdzie,'w')\n",
    "        f.write(json.dumps(co))\n",
    "    def wczytaj_json(skad):\n",
    "        f=open(skad,'r')\n",
    "        return json.loads(f.read())\n",
    "    \n",
    "    def zrob_slownik_typow(legs,jets,global_params,properties):\n",
    "        #zakladam, ze legs to jest lista o shapie (?,4) wypelniona floatami\n",
    "        #jets dokladnie tak samo\n",
    "        #global_params to jest w postaci {nazwa:liczba, ...}\n",
    "        wyrzut={}\n",
    "        legs=np.array(legs)\n",
    "        jets=np.array(jets)\n",
    "        n_legs=legs.shape[0]\n",
    "        n_jets=jets.shape[0]\n",
    "        def czy_int(x):\n",
    "            type(x)==int or type(x)==int ==numpy.int64\n",
    "\n",
    "        for i in range(n_legs):\n",
    "            wyrzut[\"leg_\"+str(i)+\"_momentum\"]=(4,'f')\n",
    "        for i in range(n_jets):\n",
    "            wyrzut[\"jet_\"+str(i)+\"_momentum\"]=(4,'f')\n",
    "        def dorob(wyrzut,global_params):\n",
    "            for param_key in global_params.keys():\n",
    "                if np.issubdtype(type(global_params[param_key]), np.integer):\n",
    "                    wyrzut[param_key]=(1,'i')\n",
    "                else:\n",
    "                    wyrzut[param_key]=(1,'f')\n",
    "            return wyrzut\n",
    "        wyrzut=dorob(wyrzut,global_params)\n",
    "        wyrzut=dorob(wyrzut,properties)\n",
    "        return wyrzut\n",
    "    def zrob_slownik_typow_old_format(f):\n",
    "        wyrzut={}\n",
    "        ff={}\n",
    "        for k in f.keys():\n",
    "            ff[k]=np.array(f[k]).reshape((-1,))\n",
    "        for k in ff.keys():\n",
    "            if np.issubdtype(type(ff[k][0]), np.integer):\n",
    "                wyrzut[k]=(len(ff[k]),'i')\n",
    "            else:\n",
    "                wyrzut[k]=(len(ff[k]),'f')\n",
    "        return wyrzut\n",
    "        \n",
    "    \n",
    "    def zrob_sensowna_forme(legs,jets,global_params,properties,l):\n",
    "        #l jest intem i to 0 lub 1\n",
    "        legs=np.array(legs)\n",
    "        jets=np.array(jets)\n",
    "        n_legs=legs.shape[0]\n",
    "        n_jets=jets.shape[0]\n",
    "\n",
    "        f={}\n",
    "\n",
    "\n",
    "        for i in range(n_legs):\n",
    "            f[\"leg_\"+str(i)+\"_momentum\"]=legs[i,:]\n",
    "        for i in range(n_jets):\n",
    "            f[\"jet_\"+str(i)+\"_momentum\"]=jets[i,:]\n",
    "\n",
    "        for param_key in global_params.keys():\n",
    "            f[param_key]=[global_params[param_key]]\n",
    "        for k in properties:\n",
    "            f[k]=[properties[k]]\n",
    "        return f,l\n",
    "    \n",
    "    \n",
    "    def zrob_sensowna_forme_general(legs_list,properties_list,l):\n",
    "        #l jest intem i to 0 lub 1\n",
    "        for i in range(len(legs_list)):\n",
    "            legs_list[i]=np.array(legs_list[i])\n",
    "#         n_legs=legs.shape[0]\n",
    "#         n_jets=jets.shape[0]\n",
    "\n",
    "        f={}\n",
    "\n",
    "        for j in range(len(legs_list)):\n",
    "            for i in range(legs_list[j].shape[0]):\n",
    "                f[\"leg_\"+str(j)+\"_\"+str(i)+\"_momentum\"]=legs_list[j][i,:]\n",
    "\n",
    "\n",
    "        for i in range(len(properties_list)):\n",
    "            properties=properties_list[i]\n",
    "            for k in properties:\n",
    "                f[k+\"_\"+str(i)]=[properties[k]]\n",
    "        return f,l\n",
    "    \n",
    "    \n",
    "    \n",
    "    def write_from_tree(self,legs,jets,global_params,properties,l,co_ile_flush_file=10,przykladowy=False):\n",
    "        #zakladam, ze legs to jest lista o shapie (?,4) wypelniona floatami\n",
    "        #jets dokladnie tak samo\n",
    "        #global_params to jest w postaci {nazwa:liczba, ...}\n",
    "        #l jest intem i to 0 lub 1\n",
    "        assert self.tryb=='w'\n",
    "#         if self.nowopowstala==True:\n",
    "#             os.system(\"mkdir \"+self.nazwa_folderu)\n",
    "#             self.nowopowstala=False\n",
    "#             slownik= Io_tf_binary_general.zrob_slownik_typow(legs,jets,global_params,properties)\n",
    "#             self.typy_pierwszego=slownik\n",
    "#             Io_tf_binary_general.zapisz_json(slownik,self.nazwa_folderu+\"/metadata\")\n",
    "#             self.stary_io=Io_tf_binary_general.Io_tf_binary_stary(\n",
    "#                 self.nazwa_folderu+\"/dane\",slownik,self.tryb,co_ile_flush_file)\n",
    "        f,l=Io_tf_binary_general.zrob_sensowna_forme(legs,jets,global_params,properties,l)\n",
    "        #slownik= Io_tf_binary_general.zrob_slownik_typow_old_format(f)\n",
    "        #assert self.typy_pierwszego==slownik\n",
    "        \n",
    "        \n",
    "        self.write_general(f,l,przykladowy=przykladowy)\n",
    "    \n",
    "    \n",
    "    def write_from_tree_general(self,legs_list,properties_list,l,co_ile_flush_file=10,przykladowy=False):\n",
    "        #zakladam, ze legs to jest lista o shapie (?,4) wypelniona floatami\n",
    "        #jets dokladnie tak samo\n",
    "        #global_params to jest w postaci {nazwa:liczba, ...}\n",
    "        #l jest intem i to 0 lub 1\n",
    "        assert self.tryb=='w'\n",
    "#         if self.nowopowstala==True:\n",
    "#             os.system(\"mkdir \"+self.nazwa_folderu)\n",
    "#             self.nowopowstala=False\n",
    "#             slownik= Io_tf_binary_general.zrob_slownik_typow(legs,jets,global_params,properties)\n",
    "#             self.typy_pierwszego=slownik\n",
    "#             Io_tf_binary_general.zapisz_json(slownik,self.nazwa_folderu+\"/metadata\")\n",
    "#             self.stary_io=Io_tf_binary_general.Io_tf_binary_stary(\n",
    "#                 self.nazwa_folderu+\"/dane\",slownik,self.tryb,co_ile_flush_file)\n",
    "        f,l=Io_tf_binary_general.zrob_sensowna_forme_general(legs_list,properties_list,l)\n",
    "        #slownik= Io_tf_binary_general.zrob_slownik_typow_old_format(f)\n",
    "        #assert self.typy_pierwszego==slownik\n",
    "        \n",
    "        \n",
    "        self.write_general(f,l,przykladowy=przykladowy)\n",
    "    \n",
    "    \n",
    "        \n",
    "    def write_general(self,features,l,co_ile_flush_file=10,przykladowy=False):\n",
    "        \n",
    "        def zrob_plik_na_niby_featchers():\n",
    "\n",
    "            with open(self.nazwa_folderu+'/on_the_fly_featcheres.pkl', 'wb') as output:\n",
    "                for eng in self.new_featcheres_na_niby_zapisz:\n",
    "                    naniby=Na_niby_featcher(eng)\n",
    "                    pickle.dump(naniby, output, pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "\n",
    "                \n",
    "        \n",
    "        \n",
    "        assert self.tryb=='w'\n",
    "        if not przykladowy:\n",
    "            if self.nowopowstala==True:\n",
    "                os.system(\"mkdir \"+self.nazwa_folderu)\n",
    "                self.nowopowstala=False\n",
    "                slownik= Io_tf_binary_general.zrob_slownik_typow_old_format(features)\n",
    "                if  (self.typy_pierwsze=={}):\n",
    "                    self.typy_pierwszego=slownik\n",
    "                else:\n",
    "                    self.typy_pierwszego=self.typy_pierwsze\n",
    "                if not self.juz_poznane:\n",
    "                    self.typy_naprawde=Io_tf_binary_general.zrob_slownik_typow_old_format(features)\n",
    "                    self.typy_naniby=self.typy_naprawde.copy()\n",
    "                    self.typy_pierwsze=self.typy_naprawde.copy()\n",
    "                Io_tf_binary_general.zapisz_json(self.typy_naprawde,self.nazwa_folderu+\"/metadata\")\n",
    "                self.stary_io=Io_tf_binary_general.Io_tf_binary_stary(\n",
    "                    self.nazwa_folderu+\"/dane\",slownik,self.tryb,co_ile_flush_file)\n",
    "                zrob_plik_na_niby_featchers()\n",
    "            slownik= Io_tf_binary_general.zrob_slownik_typow_old_format(features)\n",
    "            assert self.typy_pierwszego==slownik\n",
    "\n",
    "\n",
    "            self.stary_io.wpisz(features,l,self.new_featcheres_naprawde_zapisz)\n",
    "        else:\n",
    "            assert self.nowopowstala\n",
    "            assert self.juz_poznane==False\n",
    "            self.juz_poznane=True\n",
    "            self.przyklad=features\n",
    "            self.typy_naprawde=Io_tf_binary_general.zrob_slownik_typow_old_format(self.przyklad)\n",
    "            self.typy_naniby=self.typy_naprawde.copy()\n",
    "            self.typy_pierwsze=self.typy_naprawde.copy()\n",
    "            \n",
    "        \n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def close(self):\n",
    "        assert self.tryb=='w'\n",
    "        if not self.nowopowstala:\n",
    "            self.stary_io.close()\n",
    "    def read(self):\n",
    "        assert self.tryb=='r'\n",
    "        \n",
    "        return self.wewnetrzny.wczytaj_dataset()\n",
    "    def types(self):\n",
    "        if self.tryb=='r':\n",
    "            return self.wewnetrzny.types()\n",
    "        return self.typy_naniby\n",
    "        \n",
    "    \n",
    "    #jakby ktos kopiowal to to idzie dalej\n",
    "    #==========================================================    \n",
    "        \"\"\"\n",
    "    do creatora dajemy sobie nazwe pliku (sciezke) oraz slownik typu {'czterowektor': (4,'f'),'intowa_wlasnosc': (1,'i'),\n",
    "    ...}\n",
    "    to znaczy nazwe, ile to jest liczb, jakiego typu. Obsluguje na razie jedynie 'f' oraz 'i' to \n",
    "    jest float oraz int\n",
    "    nie mozna uzyc jako klucz 'label', bo to jest wykorzystywana nazwa.\n",
    "\n",
    "\n",
    "    funkcja wpisz bierze jako argument jeden przypadek  cos typu \n",
    "    ({\"czterowektor\":[1.,2.,3.,4.], ...},1) gdzie 1 jest labelem, label jest intem.\n",
    "    . oczywiście klucze slownika zgadzają się \n",
    "    z kluczami ze slownika ktorego uzylismy do kreatora.\n",
    "\n",
    "    dataset wczytany metoda wczytaj dataset jest juz w postaci wygodnej dla mnie to znaczy \n",
    "    slownik feature, label\n",
    "    \"\"\"\n",
    "    class Io_tf_binary_stary:\n",
    "        def __init__(self,nazwa_pliku,slownik,tryb,co_ile_flush_file=10):\n",
    "            #tryb to 'w' dla write oraz 'r' dla read\n",
    "            \n",
    "            self.co_ile=co_ile_flush_file\n",
    "            self.plik=nazwa_pliku\n",
    "            self.typy=slownik\n",
    "            for k in self.typy.keys():\n",
    "                assert self.typy[k][1] in ['f','i']\n",
    "                assert self.typy[k][0]>0 \n",
    "                assert np.issubdtype(type(self.typy[k][0]), np.integer)\n",
    "            self.tryb=tryb\n",
    "            if tryb=='w':\n",
    "                self.writer= tf.python_io.TFRecordWriter(self.plik)\n",
    "            if tryb=='r':\n",
    "                self.dataset=self.wczytaj_bez_feature_engeeneringu()\n",
    "            self.liczba_wrzuconych=0\n",
    "\n",
    "            #self.cos=Io_tf_binary.wrap_int64([5])\n",
    "        def types(self):\n",
    "            return self.typy\n",
    "\n",
    "        def close(self):\n",
    "            self.writer.close()\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        To moze sobie czytac ktos kto chce zmieniac wnetrznosci tej klasy\n",
    "        Nie polecam\n",
    "\n",
    "        Teraz ta funkcja wpisz jest ważna. ona bierze po jednym przypadku testowym, \n",
    "        ( to jest ta petla for i in range()) i go zapisuje. trzeba zwracac \n",
    "        uwage na to jakiego typu sa zapisywane rzeczy. mozna oczywiscie zrobic slownik\n",
    "        data dluzszym, jesli to w jakis sposob ulatwi nam myslenie o naszych danych. \n",
    "        Bo te nasze dane to bedzie slownik list, w ktorych to listach rzeczy maja \n",
    "        juz taki sam typ, a klucze to beda jakies opisowe nazwy.\n",
    "        np \n",
    "\n",
    "        data={\n",
    "        'czteroped_lewej_nogi_czy_cos': wrap_float64(cztero), # gdzie cztero to jest tensor floatow o shape (4,)\n",
    "        # reszta rzeczy\n",
    "\n",
    "        }\n",
    "\n",
    "        Jak byscie chcieli jako wartosci miec stringi to musicie pomyslec jak zrobic wrapy dla stringow. oczywiscie\n",
    "        nie znajdziecie zadnej dokumentacji.\n",
    "\n",
    "        UWAGA \n",
    "        w tym slowniku data musi byc to co klasyfikujemy oznaczone przy pomocy 'label' bo inaczej sie  wywali program.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        def wpisz(self,features,label,new_featcheres):\n",
    "            \"\"\"tworzy ten nasz dataset w pliku out_path \n",
    "            tu format jest taki jak byl na poczatku to znaczy taki slownik\"\"\"\n",
    "            f=features\n",
    "            l=label\n",
    "            \n",
    "            for fet in new_featcheres:\n",
    "                podstawienia={}\n",
    "                for k in fet['slownik'].keys():\n",
    "                    podstawienia[k]=f[fet['slownik'][k]]\n",
    "                f[fet['nazwa']]=fet['f'](**podstawienia)\n",
    "                self.typy[fet['nazwa']]=fet['typ']\n",
    "            \n",
    "            \n",
    "            def wrap_int64(value):\n",
    "                \"\"\"lista intow musi wlesc\"\"\"\n",
    "                return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "            def wrap_float64(value):\n",
    "                \"\"\"lista floatow musi wlesc\"\"\"\n",
    "                return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "            #f,l=kolko_w_kolku() #mozna zmienic jak sie podoba\n",
    "            def data_slownik(f,l):\n",
    "                wyrzut={}\n",
    "                for k in self.typy.keys():\n",
    "                    if self.typy[k][1]=='f':\n",
    "                        wyrzut[k]=wrap_float64(np.array(f[k]).reshape((-1,)))\n",
    "                    else:\n",
    "                        wyrzut[k]=wrap_int64(np.array(f[k]).reshape((-1,)))\n",
    "                wyrzut['label']=wrap_int64([l])\n",
    "                return wyrzut\n",
    "\n",
    "\n",
    "\n",
    "            #feature=f[i]\n",
    "            #label=l[i]\n",
    "            #data = {\n",
    "             #    'feature': wrap_float64(feature),\n",
    "            #  'label': wrap_int64([label])\n",
    "               #     }\n",
    "            data=data_slownik(f,l)\n",
    "            # Wrap the data as TensorFlow Features.\n",
    "            feature = tf.train.Features(feature=data)\n",
    "\n",
    "            # Wrap again as a TensorFlow Example.\n",
    "            example = tf.train.Example(features=feature)\n",
    "\n",
    "            # Serialize the data.\n",
    "            serialized = example.SerializeToString()\n",
    "\n",
    "            # Write the serialized data to the TFRecords file.\n",
    "            self.writer.write(serialized)\n",
    "            self.liczba_wrzuconych+=1\n",
    "            if self.liczba_wrzuconych%self.co_ile==0:\n",
    "                self.writer.flush()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        def wczytaj_bez_feature_engeeneringu(self):\n",
    "\n",
    "            def zeslownikoj(x):\n",
    "                keys=list(x.keys())\n",
    "                f={}\n",
    "                for k in keys:\n",
    "                    if not k=='label':\n",
    "                        f[k]=x[k]\n",
    "                return f,x['label']\n",
    "            def features_generoj():\n",
    "                wyrzut={}\n",
    "                for k in self.typy.keys():\n",
    "                    if self.typy[k][1]=='f':\n",
    "                        wyrzut[k]=tf.FixedLenFeature([self.typy[k][0]], tf.float32)\n",
    "                    else:\n",
    "                        wyrzut[k]=tf.FixedLenFeature([self.typy[k][0]], tf.int64)\n",
    "                wyrzut['label']=tf.FixedLenFeature([], tf.int64)\n",
    "                return wyrzut\n",
    "\n",
    "            def parse(serialized):\n",
    "\n",
    "                # Define a dict with the data-names and types we expect to\n",
    "                # find in the TFRecords file.\n",
    "                # It is a bit awkward that this needs to be specified again,\n",
    "                # because it could have been written in the header of the\n",
    "                # TFRecords file instead.\n",
    "                \"\"\"\n",
    "                features = \\\n",
    "                    {\n",
    "                        'dwuwektor': tf.FixedLenFeature([2], tf.float32),#z jakiegos powodu to jest float32, nie wiem czemu\n",
    "                        'label': tf.FixedLenFeature([], tf.int64)\n",
    "                    }\n",
    "                \"\"\"\n",
    "                features=features_generoj()\n",
    "                print(features)\n",
    "\n",
    "                # Parse the serialized data so we get a dict with our data.\n",
    "                parsed_example = tf.parse_single_example(serialized=serialized,\n",
    "                                                         features=features)\n",
    "\n",
    "\n",
    "                return zeslownikoj(parsed_example)\n",
    "\n",
    "            dataset = tf.data.TFRecordDataset(self.plik)\n",
    "            dataset = dataset.map(parse)\n",
    "            return dataset\n",
    "\n",
    "\n",
    "        def engineer_feature(self,f,slownik,typ,nazwa):\n",
    "            print(\"taki tam engeenerowany featcher \")\n",
    "            print(nazwa)\n",
    "            print(typ)\n",
    "            print(slownik)\n",
    "            assert self.tryb=='r'\n",
    "            \"\"\" to ma zmienic po prostu nasz self.dataset\"\"\"\n",
    "            def dodaj_jeden_feature(engineered,dataset):\n",
    "                \"\"\"ten engineered to jest ten slownik {'f':f,'slownik':slownik,'typ':typ,'nazwa':nazwa}\"\"\"\n",
    "                for k in engineered['slownik'].keys():\n",
    "                    assert engineered['slownik'][k] in self.typy.keys()\n",
    "                assert not (engineered['nazwa'] in self.typy.keys())\n",
    "                def lambdowata(f,label):\n",
    "                    #features,label=jeden_przyklad\n",
    "                    features=f.copy()\n",
    "                    def zrob_podstawienie():\n",
    "                        podstawienie={}\n",
    "                        for zmienna in engineered['slownik'].keys():\n",
    "                            podstawienie[zmienna]=features[engineered['slownik'][zmienna]]\n",
    "                        return podstawienie\n",
    "                    nazwa=engineered['nazwa']\n",
    "                    features[nazwa]=engineered['f'](**zrob_podstawienie())\n",
    "                    self.typy[nazwa]=engineered['typ']\n",
    "                    return features,label\n",
    "                return dataset.map(lambdowata)\n",
    "                    \n",
    "            engi={'f':f,'slownik':slownik,'typ':typ,'nazwa':nazwa}\n",
    "            self.dataset= dodaj_jeden_feature(engi,self.dataset)\n",
    "        def wczytaj_dataset(self):\n",
    "            assert self.tryb=='r'\n",
    "            return self.dataset\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "pisacz=Io_tf_binary_general(\"proby\",'w')\n",
    "przyklad={}\n",
    "przyklad[\"jeden\"]=np.random.randn(3)\n",
    "przyklad[\"dwa\"]=np.random.randn(7)\n",
    "przyklad[\"jedynka\"]=10\n",
    "pisacz.write_general(przyklad,1,przykladowy=True)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(j,d):\n",
    "    return tf.reshape(tf.reduce_mean(tf.cast(j,dtype=tf.float32)+(100*d)),(-1,))\n",
    "pisacz.engineer_feature(f,slownik={'j':'jedynka','d':'dwa'},typ=(1,'f'),nazwa=\"nowiutki\",naprawde_zapisz=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01669818, -2.04051004, -0.45634536])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randn(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pisacz=Io_tf_binary_general(\"proby\",'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘proby’: File exists\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    przyklad={}\n",
    "    przyklad[\"jeden\"]=np.random.randn(3)\n",
    "    przyklad[\"dwa\"]=np.random.randn(7)\n",
    "    przyklad[\"jedynka\"]=10\n",
    "    pisacz.write_general(przyklad,1)\n",
    "pisacz.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'jeden': FixedLenFeature(shape=[3], dtype=tf.float32, default_value=None), 'dwa': FixedLenFeature(shape=[7], dtype=tf.float32, default_value=None), 'jedynka': FixedLenFeature(shape=[1], dtype=tf.int64, default_value=None), 'label': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None)}\n",
      "taki tam engeenerowany featcher \n",
      "nowiutki\n",
      "(1, 'f')\n",
      "{'j': 'jedynka', 'd': 'dwa'}\n"
     ]
    }
   ],
   "source": [
    "czytacz=Io_tf_binary_general(\"proby\",'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'jeden': [3, 'f'], 'dwa': [7, 'f'], 'jedynka': [1, 'i'], 'nowiutki': (1, 'f')}"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "czytacz.types()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def funkcja(j,d):\n",
    "    return (tf.reduce_sum(j)+tf.reduce_sum(d))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taki tam engeenerowany featcher \n",
      "nowy_smieszny\n",
      "[1, 'f']\n",
      "{'j': 'jeden', 'd': 'dwa'}\n"
     ]
    }
   ],
   "source": [
    "czytacz.engineer_feature(funkcja,{'j':'jeden','d':\"dwa\"},[1,'f'],'nowy_smieszny')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'jeden': [3, 'f'],\n",
       " 'dwa': [7, 'f'],\n",
       " 'jedynka': [1, 'i'],\n",
       " 'nowiutki': (1, 'f'),\n",
       " 'nowy_smieszny': [1, 'f']}"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "czytacz.types()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "({'dwa': array([[-0.23448572,  1.1717988 , -0.34003028,  0.12781341, -0.06296232,\n",
      "        -1.2120279 , -0.06733961]], dtype=float32), 'jeden': array([[-0.6199144 ,  0.04860768, -0.90496695]], dtype=float32), 'jedynka': array([[10]]), 'nowiutki': array([[1.1823777]], dtype=float32), 'nowy_smieszny': array([-2.0935073], dtype=float32)}, array([1]))\n",
      "({'dwa': array([[ 2.5854328 , -0.01093143,  1.3227022 , -0.23763637, -0.30917117,\n",
      "        -1.6458774 ,  1.1543059 ]], dtype=float32), 'jeden': array([[-1.1924336, -1.3015646,  1.2048196]], dtype=float32), 'jedynka': array([[10]]), 'nowiutki': array([[50.84035]], dtype=float32), 'nowy_smieszny': array([1.569646], dtype=float32)}, array([1]))\n",
      "({'dwa': array([[ 2.5854328 , -0.01093143,  1.3227022 , -0.23763637, -0.30917117,\n",
      "        -1.6458774 ,  1.1543059 ]], dtype=float32), 'jeden': array([[-1.1924336, -1.3015646,  1.2048196]], dtype=float32), 'jedynka': array([[10]]), 'nowiutki': array([[50.84035]], dtype=float32), 'nowy_smieszny': array([1.569646], dtype=float32)}, array([1]))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset=czytacz.read()\n",
    "BATCH_SIZE=1\n",
    "zbachowany=dataset.shuffle(1000).repeat().batch(BATCH_SIZE)\n",
    "iterator = zbachowany.make_one_shot_iterator()\n",
    "para=iterator.get_next()\n",
    "for i in range(10):\n",
    "    print(\" \")\n",
    "with tf.Session() as sess:\n",
    "    for i in range(3):\n",
    "        print(sess.run(para))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ML]\tOpening root file for python conversion.\n",
      "[ML]\tReading data from TTree.\n",
      "[ML]\tConversion to python successful!\n"
     ]
    }
   ],
   "source": [
    "in_file=\"data/dummy.root\"\n",
    "out_file=\"output/example\"\n",
    "tree_path=\"Summary/tree\"\n",
    "legs, jets, global_params, properties = rt.read_tree(in_file, tree_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "637"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(legs).shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wybierz(slownik,i):\n",
    "    wyrzut={}\n",
    "    for k in slownik.keys():\n",
    "        wyrzut[k]=slownik[k][i]\n",
    "    return wyrzut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#poczatek przykladu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "pisacz=Io_tf_binary_general(\"pierwszy\",'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘pierwszy’: File exists\n"
     ]
    }
   ],
   "source": [
    "for i in range(np.array(legs).shape[2]):\n",
    "    pisacz.write_from_tree(np.array(legs)[:,:,i],np.array(jets)[:,:,i],wybierz(global_params,i),wybierz(properties,i),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "pisacz.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'leg_0_momentum': FixedLenFeature(shape=[4], dtype=tf.float32, default_value=None), 'leg_1_momentum': FixedLenFeature(shape=[4], dtype=tf.float32, default_value=None), 'jet_0_momentum': FixedLenFeature(shape=[4], dtype=tf.float32, default_value=None), 'jet_1_momentum': FixedLenFeature(shape=[4], dtype=tf.float32, default_value=None), 'BJetBetaScore': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None), 'higgsMassTrans': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None), 'higgsPT': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None), 'nJets30': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None), 'visMass': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None), 'leg_1_charge': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None), 'leg_1_combreliso': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None), 'leg_2_byCombinedIsolationDeltaBetaCorrRaw3Hits': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None), 'leg_2_byIsolationMVArun2v1DBoldDMwLTraw': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None), 'leg_2_charge': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None), 'leg_2_decayMode': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None), 'label': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None)}\n"
     ]
    }
   ],
   "source": [
    "czytacz=Io_tf_binary_general(\"pierwszy\",'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=czytacz.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'leg_0_momentum': [4, 'f'],\n",
       " 'leg_1_momentum': [4, 'f'],\n",
       " 'jet_0_momentum': [4, 'f'],\n",
       " 'jet_1_momentum': [4, 'f'],\n",
       " 'BJetBetaScore': [1, 'f'],\n",
       " 'higgsMassTrans': [1, 'f'],\n",
       " 'higgsPT': [1, 'f'],\n",
       " 'nJets30': [1, 'f'],\n",
       " 'visMass': [1, 'f'],\n",
       " 'leg_1_charge': [1, 'f'],\n",
       " 'leg_1_combreliso': [1, 'f'],\n",
       " 'leg_2_byCombinedIsolationDeltaBetaCorrRaw3Hits': [1, 'f'],\n",
       " 'leg_2_byIsolationMVArun2v1DBoldDMwLTraw': [1, 'f'],\n",
       " 'leg_2_charge': [1, 'f'],\n",
       " 'leg_2_decayMode': [1, 'f']}"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "czytacz.types()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'BJetBetaScore': array([[-999.],\n",
      "       [-999.]], dtype=float32), 'higgsMassTrans': array([[73.90409],\n",
      "       [64.90739]], dtype=float32), 'higgsPT': array([[97.62944],\n",
      "       [70.70418]], dtype=float32), 'jet_0_momentum': array([[ 111.46314 ,   12.080458,   23.129747, -108.30056 ],\n",
      "       [ 424.6742  ,   62.065292,  128.02869 ,  399.4694  ]],\n",
      "      dtype=float32), 'jet_1_momentum': array([[0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0.]], dtype=float32), 'leg_0_momentum': array([[192.45575  ,  78.14696  , -48.799053 , 168.97014  ],\n",
      "       [ 27.462416 ,  -7.2122245, -23.416668 , -12.40228  ]],\n",
      "      dtype=float32), 'leg_1_charge': array([[-1.],\n",
      "       [-1.]], dtype=float32), 'leg_1_combreliso': array([[0.        ],\n",
      "       [0.08552033]], dtype=float32), 'leg_1_momentum': array([[ 21.561855 , -17.338188 , -12.745424 ,   1.0068885],\n",
      "       [ 53.49213  ,  11.292852 ,  24.43085  , -46.2181   ]],\n",
      "      dtype=float32), 'leg_2_byCombinedIsolationDeltaBetaCorrRaw3Hits': array([[0.       ],\n",
      "       [1.3943781]], dtype=float32), 'leg_2_byIsolationMVArun2v1DBoldDMwLTraw': array([[0.9022239],\n",
      "       [0.9393842]], dtype=float32), 'leg_2_charge': array([[1.],\n",
      "       [1.]], dtype=float32), 'leg_2_decayMode': array([[10.],\n",
      "       [10.]], dtype=float32), 'nJets30': array([[1.],\n",
      "       [2.]], dtype=float32), 'visMass': array([[97.08714],\n",
      "       [55.67413]], dtype=float32)}, array([0, 0]))\n",
      "({'BJetBetaScore': array([[-9.9900000e+02],\n",
      "       [ 9.9800676e-01]], dtype=float32), 'higgsMassTrans': array([[86.212494],\n",
      "       [65.52654 ]], dtype=float32), 'higgsPT': array([[42.575447],\n",
      "       [43.015907]], dtype=float32), 'jet_0_momentum': array([[  0.       ,   0.       ,   0.       ,   0.       ],\n",
      "       [ 29.34198  ,   6.6082277,  25.736082 , -10.9442425]],\n",
      "      dtype=float32), 'jet_1_momentum': array([[0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0.]], dtype=float32), 'leg_0_momentum': array([[115.874725 ,  82.69402  , -16.97495  ,  79.37563  ],\n",
      "       [ 38.938408 ,   6.4620557,  24.692974 ,  29.405567 ]],\n",
      "      dtype=float32), 'leg_1_charge': array([[1.],\n",
      "       [1.]], dtype=float32), 'leg_1_combreliso': array([[0.00772537],\n",
      "       [0.12185193]], dtype=float32), 'leg_1_momentum': array([[ 34.341454, -28.854038,  10.573386,  15.318067],\n",
      "       [ 31.967634,  20.123419, -14.568716, -20.062626]], dtype=float32), 'leg_2_byCombinedIsolationDeltaBetaCorrRaw3Hits': array([[0.],\n",
      "       [0.]], dtype=float32), 'leg_2_byIsolationMVArun2v1DBoldDMwLTraw': array([[0.913877 ],\n",
      "       [0.9766466]], dtype=float32), 'leg_2_charge': array([[-1.],\n",
      "       [-1.]], dtype=float32), 'leg_2_decayMode': array([[1.],\n",
      "       [1.]], dtype=float32), 'nJets30': array([[0.],\n",
      "       [1.]], dtype=float32), 'visMass': array([[103.238945],\n",
      "       [ 64.273544]], dtype=float32)}, array([0, 0]))\n",
      "({'BJetBetaScore': array([[-999.],\n",
      "       [-999.]], dtype=float32), 'higgsMassTrans': array([[77.46321],\n",
      "       [46.01124]], dtype=float32), 'higgsPT': array([[87.753685],\n",
      "       [16.127096]], dtype=float32), 'jet_0_momentum': array([[ 583.79486 ,  -43.600735,   35.531628, -580.93616 ],\n",
      "       [ 149.26878 ,   13.003767,  -19.532925,  147.28165 ]],\n",
      "      dtype=float32), 'jet_1_momentum': array([[0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0.]], dtype=float32), 'leg_0_momentum': array([[216.21725  , 104.2906   ,   3.0090117, 189.37874  ],\n",
      "       [ 67.32849  , -15.160804 ,  18.403374 ,  62.964916 ]],\n",
      "      dtype=float32), 'leg_1_charge': array([[-1.],\n",
      "       [-1.]], dtype=float32), 'leg_1_combreliso': array([[0.       ],\n",
      "       [0.0645634]], dtype=float32), 'leg_1_momentum': array([[  32.75319 ,  -12.759178,   16.441757,  -25.290789],\n",
      "       [ 113.76886 ,  -17.357418,  -17.050037, -111.1324  ]],\n",
      "      dtype=float32), 'leg_2_byCombinedIsolationDeltaBetaCorrRaw3Hits': array([[0.        ],\n",
      "       [0.61256576]], dtype=float32), 'leg_2_byIsolationMVArun2v1DBoldDMwLTraw': array([[0.8814455 ],\n",
      "       [0.90837395]], dtype=float32), 'leg_2_charge': array([[1.],\n",
      "       [1.]], dtype=float32), 'leg_2_decayMode': array([[ 0.],\n",
      "       [10.]], dtype=float32), 'nJets30': array([[2.],\n",
      "       [1.]], dtype=float32), 'visMass': array([[162.18845],\n",
      "       [171.51349]], dtype=float32)}, array([0, 0]))\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE=2\n",
    "zbachowany=dataset.shuffle(1000).repeat().batch(BATCH_SIZE)\n",
    "iterator = zbachowany.make_one_shot_iterator()\n",
    "para=iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for i in range(3):\n",
    "        print(sess.run(para))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "pisacz=Io_tf_binary_general(\"pierwszy\",'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘pierwszy’: File exists\n"
     ]
    }
   ],
   "source": [
    "for i in range(np.array(legs).shape[2]):\n",
    "    pisacz.write_from_tree_general([np.array(legs)[:,:,i],np.array(jets)[:,:,i]],[wybierz(global_params,i),wybierz(properties,i)],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "pisacz.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
