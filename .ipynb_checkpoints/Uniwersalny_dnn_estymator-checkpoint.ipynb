{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#teraz sproboje z tego zrobic jakis dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f,l=kolko_w_kolku()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = tf.data.Dataset.from_tensor_slices((f,l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BATCH_SIZE=100\n",
    "#zbachowany=dataset.shuffle(1000).repeat().batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to jest dataset zaraz zastanowie sie jak go zapisac do pliku i z tego pliku odczytac a teraz jak go czytac?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterator = zbachowany.make_one_shot_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f,l=iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with tf.Session() as sess:\n",
    "#    for i in range(100):\n",
    "#        sess.run(l)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wrap_float64([5.55,6.85]) #swietnie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.15/01\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "Tym razem jak podajemy sciezke do folderu to ten plik ma juz byc pelen danych. \n",
    "\n",
    "bardzo nie lubi jak mu sie przerywa trening przy pomocy kernel interrrupt\n",
    "pozniej nie chodza rozne rzeczy w takim przerwanym obiekcie\n",
    "bo jak zaczyna trening to finalizuje graph co kolwiek to znacyzy, i \n",
    "jak konczy to chyba go odfinalizowywuje, ale jak przerwiemy to tego nie zrobi. nie do \n",
    "konca to rozumiem. wiec nie przerywamy treningu ani niczego inngo.\n",
    "\n",
    "Na razie w pierwszym rzucie nie bedzie jeszcze danych typu kategorycznego.\n",
    "\n",
    "To ma byc kompatybilne z klasa Io_tf_binary_general\n",
    "\n",
    "\n",
    "\n",
    "__init__(nazwa_folderu,hidden_units,model_dir,czy_nowy=True)\n",
    "        to jest nazwa_folderu odnosi sie do folderu w ktorym pisala klasa\n",
    "        Io_tf_binary_general czy cos\n",
    "        hidden_units to jest lista po ile ma byc ukrytych unitsow. czyli nie podajemy rozmiaru\n",
    "        danych wejsciowych ani wyjsciowych. idziemy od pierwszej (najblizszej inputu) do ostatniej\n",
    "        model_dir to tam bedzie pisac swoje rzeczy nasz model\n",
    "        czy_nowy: to znaczy, czy tworzymy nowy model czy probojemy wczytac model ktory\n",
    "        kiedys sie liczyl i ma juz dosyc dobre wagi? (sa subtelnosci z uzywaniem\n",
    "        enginner_featcher na poziomie tego uniwersalnego dnn_estimatora, to znaczy\n",
    "        trzeba dac takie same enginner featcher jak ma dzialac.)\n",
    "engineer_feature(self,f,slownik,typ,nazwa)\n",
    "        dokladnie jak w tamtym dla io_tf_binary_general\n",
    "make_model(self,cathegorical_vocabulary_lists):\n",
    "        to tworzy nasz model po tym jak podawalismy transformacje dla danych zgodnie z \n",
    "        engineer_feature() (taka jest metoda)\n",
    "        tutaj cathegorical_vocabulary_lists to jest slownik typu\n",
    "        {'jakies_id':[1,2,3]} to znaczy, ze takie cos moze miec takie wartosci. w kluczach\n",
    "        sa tylko kategoryczne argumenty. Wazne, ze to maja byc integery a nie na przyklad\n",
    "        floaty czy inne takie. stringow tez nie obsluguję. \n",
    "        \n",
    "        Teraz dodaje normalizacje danych wejsciowych. odbedzie sie w trakcie wykonywania\n",
    "        make_model. Normalizuje tylko dane floatowe, zakladajac, ze te intowe \n",
    "        to sa jakies znormalizowane.\n",
    "train \n",
    "        jest self explainatory. wydaje mi sie, ze to robi tak, ze kontynuuje\n",
    "        trenowanie z miejsca w ktorym skonczylo\n",
    "evaluate \n",
    "        to jest zwykla ewaluacja. tyle, ze mozna podac jako argument \"folder\" z ktorego pochodza nasze dane.\n",
    "        ale ta funkcja zawsze dziala tak, ze po prostu traktuje 1 jako prawdziwe przypadki a \n",
    "        0 jako tlo\n",
    "ma tez rysowac roc czy cos takiego. \n",
    "niech evaluate daje nam zbior punktow na krzywej roc a potem \n",
    "niech bedzie metoda rotate na przyklad ktora nam zrobi krzywa roc dla odrozniania prawdziwych klas\n",
    "evaluate_jak_z_pracy(self,p0to1,p1to1,ile_take=10000,folder=\"\")\n",
    "        to robi takie szacowanie krzywej roc oraz auc wartosci jak w tej pracy. tutaj podajemy\n",
    "        jako folder te nasze przypadki. ile_take to znaczy jak wiele przypadkow z tego datasetu\n",
    "         z argumentu 'folder' nalezy wziasc. p0to1 to jest prawdopodoienstwo, ze przypadek oznaczony \n",
    "         0 jest tak na prawde 1 zas p1to1 to jest ze oznaczony jako 1 jest tak naprawde wlasnie rozpadem \n",
    "         czyli 1.zwraca jako pierwsze auc a potem jeszcze 3 listy zwiazane z rysowaniem krzywej roc.\n",
    "         to znaczy liste jej xsow, jej ygrekow oraz jakim tresholdom te punkty krzywej roc odpowiadaja.\n",
    "types()\n",
    "        to jest zmienna tego obiektu ktora ma informacje o tym jakie feateres sa w naszym datasecie\n",
    "        po tym jak zrobisz feature engineering to sie zmieni wynik podzialania .types()\n",
    "\n",
    "        \n",
    "niech w trakcie train niech pojawi sie accuracy co step.\n",
    "mozna by sprobopwac tensorboarda. dodac do tensorboard zmienne do monitorowania.\n",
    "Tu jest taki artefakt, ze ten area under curve czasem jest więcej niz 1 i to nie za dobrze.\n",
    ".\n",
    "Dodaj normalizacje oraz zapisz po tym jak przeleci przez dane treningowe. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import Io_tf_binary_general as io\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "class Na_niby_featcher(object):\n",
    "    def __init__(self, eng):\n",
    "        self.eng=eng\n",
    "    def to_dict(self):\n",
    "        return self.eng\n",
    "\n",
    "class Dnn_uniwersalny:\n",
    "    def __init__(self,nazwa_folderu,hidden_units,model_dir,czy_nowy=True):\n",
    "        if czy_nowy:\n",
    "            assert not os.path.isdir(model_dir)\n",
    "            \n",
    "        \n",
    "        self.not_compiled=True\n",
    "        if not czy_nowy:\n",
    "            self.not_compiled=False\n",
    "        self.model_dir=model_dir\n",
    "        self.nazwa_folderu=nazwa_folderu\n",
    "        os.system(\"mkdir \"+model_dir)\n",
    "        self.hidden_units=hidden_units\n",
    "        self.wczytywacz=io.Io_tf_binary_general(nazwa_folderu,'r')\n",
    "        self.lista_transformacji=[]\n",
    "        self.czy_nowy=czy_nowy\n",
    "        self.czy_wczytane_typy=False\n",
    "\n",
    "    def licz_normalizacje(wczytywacz,na_ilu):\n",
    "        dataset=wczytywacz.read()\n",
    "        typy=wczytywacz.types()\n",
    "\n",
    "        zbachowany=dataset.repeat().batch(na_ilu)\n",
    "        iterator = zbachowany.make_one_shot_iterator()\n",
    "        para=iterator.get_next()\n",
    "        def mean_var(para,k):\n",
    "            return tf.nn.moments(para[0][k],axes=[0])\n",
    "            # gdzie var to jest to podniesione do kwadratu cos\n",
    "        mean_slownik={}\n",
    "        sig_slownik={}\n",
    "        for k in typy.keys():\n",
    "            if typy[k][1]=='f':\n",
    "                mean,var=mean_var(para,k)\n",
    "                mean_slownik[k]=mean\n",
    "                sig_slownik[k]=var**(0.5)\n",
    "        return mean_slownik,sig_slownik\n",
    "\n",
    "    def zapisz_json(co,gdzie):\n",
    "        f=open(gdzie,'w')\n",
    "        f.write(json.dumps(co))\n",
    "    def wczytaj_json(skad):\n",
    "        f=open(skad,'r')\n",
    "        return json.loads(f.read())\n",
    "    def odczytaj_na_niby_featcheres(self):\n",
    "        with open(self.model_dir+'/new_featchers.pkl', 'rb') as input:\n",
    "            wyrzut=[]\n",
    "            rob=True\n",
    "            while rob:\n",
    "                try:\n",
    "                    wyrzut.append(pickle.load(input).to_dict())\n",
    "                except:\n",
    "                    rob=False\n",
    "            return wyrzut\n",
    "    def zrob_plik_na_niby_featchers(self):\n",
    "\n",
    "        with open(self.model_dir+'/new_featchers.pkl', 'wb') as output:\n",
    "            for eng in self.lista_transformacji:\n",
    "                naniby=Na_niby_featcher(eng)\n",
    "                pickle.dump(naniby, output, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    def make_model(self,cathegorical_vocabulary_lists={},na_ilu_liczyc_mean_oraz_sigma=10000):\n",
    "        if not self.czy_nowy:\n",
    "            assert cathegorical_vocabulary_lists=={}\n",
    "        assert self.not_compiled or (not self.czy_nowy)\n",
    "        if self.czy_nowy:\n",
    "            self.zrob_plik_na_niby_featchers()\n",
    "            Dnn_uniwersalny.zapisz_json(cathegorical_vocabulary_lists,self.model_dir+\"/cathegorical\")\n",
    "            Dnn_uniwersalny.zapisz_json(self.wczytywacz.types(),self.model_dir+'/typy')\n",
    "        cathegorical_vocabulary_lists=Dnn_uniwersalny.wczytaj_json(self.model_dir+\"/cathegorical\")\n",
    "        self.czy_wczytane_typy=True\n",
    "        self.typy_wczytane=Dnn_uniwersalny.wczytaj_json(self.model_dir+\"/typy\")\n",
    "        \n",
    "        my_feature_columns = []\n",
    "        for k in self.typy_wczytane.keys():\n",
    "            if not( k in cathegorical_vocabulary_lists.keys()):\n",
    "                if self.typy_wczytane[k][1]=='f':\n",
    "                    t=tf.float32\n",
    "                else:\n",
    "                    t=tf.int32\n",
    "                my_feature_columns.append(tf.feature_column.numeric_column(key=k,shape=\\\n",
    "                            (self.typy_wczytane[k][0],),dtype=t ))\n",
    "                \n",
    "            else:\n",
    "                assert self.typy_wczytane[k][1]=='i'\n",
    "                my_feature_columns.append(\n",
    "                    \n",
    "                    tf.feature_column.embedding_column(\n",
    "                    \n",
    "    tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    key=k,vocabulary_list=cathegorical_vocabulary_lists[k],\n",
    "    ),len(cathegorical_vocabulary_lists[k])))\n",
    "        self.feature_columns=my_feature_columns\n",
    "        \n",
    "        if self.czy_nowy:\n",
    "            dict_of_means,dict_of_sigmas=Dnn_uniwersalny.licz_normalizacje(self.wczytywacz,\n",
    "                                na_ilu_liczyc_mean_oraz_sigma)\n",
    "            with tf.Session() as sess:\n",
    "                def zlistoj(slownik):\n",
    "                    wyrzut=slownik.copy()\n",
    "                    for k in slownik.keys():\n",
    "                        wyrzut[k]=list(slownik[k])\n",
    "                        for i in range(len(wyrzut[k])):\n",
    "                            wyrzut[k][i]=float(wyrzut[k][i])\n",
    "                    return wyrzut\n",
    "                Dnn_uniwersalny.zapisz_json(zlistoj(sess.run(dict_of_means)),self.model_dir+\"/means\")\n",
    "                Dnn_uniwersalny.zapisz_json(zlistoj(sess.run(dict_of_sigmas)),self.model_dir+\"/sigmas\")\n",
    "        \n",
    "        def znumpyuj(slownik):\n",
    "            wyrzut=slownik.copy()\n",
    "            for k in slownik.keys():\n",
    "                wyrzut[k]=np.array(slownik[k])\n",
    "            return wyrzut\n",
    "        dict_of_means=znumpyuj(Dnn_uniwersalny.wczytaj_json(self.model_dir+'/means'))\n",
    "        dict_of_sigmas=znumpyuj(Dnn_uniwersalny.wczytaj_json(self.model_dir+'/sigmas'))\n",
    "            \n",
    "        self.lista_transformacji_wczytana=self.odczytaj_na_niby_featcheres()\n",
    "            \n",
    "        def input_fn( batch_size=100,buffer_size=1000,folder=self.nazwa_folderu,one_epoch=False,czy_shuffle=True\n",
    "                    ,czy_batch=True,take=False,ile_take=10000,kategoryczne=cathegorical_vocabulary_lists.keys(),\n",
    "                    lista_transformacji=self.lista_transformacji_wczytana,dict_of_means=dict_of_means,\n",
    "                    dict_of_sigmas=dict_of_sigmas):\n",
    "            \"\"\"input function for training\n",
    "            nazwy to lista nazw feature w kolejnosci wystepowania\n",
    "            if one_epoch to tylko jedna epoka\"\"\"\n",
    "            io_gen=io.Io_tf_binary_general(folder,'r')\n",
    "            \n",
    "            for tran in lista_transformacji:\n",
    "                io_gen.engineer_feature(**tran)\n",
    "            dataset=io_gen.read()\n",
    "                \n",
    "            def fun(f,l):\n",
    "                wyrzut=f.copy()\n",
    "                for k in kategoryczne:\n",
    "                    wyrzut[k]=tf.reshape(f[k],shape=[])\n",
    "                return wyrzut,l\n",
    "            def normalizacja(f,l):\n",
    "                wyrzut=f.copy()\n",
    "                for k in dict_of_means.keys():\n",
    "                    wyrzut[k]=(f[k]-dict_of_means[k])/dict_of_sigmas[k]\n",
    "                return wyrzut,l\n",
    "                \n",
    "            \n",
    "            dataset=dataset.map(fun)\n",
    "            dataset=dataset.map(normalizacja)\n",
    "            if take:\n",
    "                dataset=dataset.take(ile_take)\n",
    "            if czy_shuffle:\n",
    "                dataset=dataset.shuffle(buffer_size)\n",
    "            if one_epoch:\n",
    "                dataset=dataset.repeat(1)\n",
    "            else:\n",
    "                dataset=dataset.repeat()\n",
    "            if czy_batch:\n",
    "                dataset=dataset.batch(batch_size)\n",
    "\n",
    "            return dataset\n",
    "        self.input_fn=input_fn\n",
    "        \n",
    "        self.classifier = tf.estimator.DNNClassifier(\n",
    "        feature_columns=self.feature_columns,\n",
    "        hidden_units=self.hidden_units,\n",
    "        model_dir=self.model_dir+'/tensorflowowy',\n",
    "        n_classes=2)\n",
    "        self.not_compiled=False\n",
    "    def types(self):\n",
    "        if self.czy_wczytane_typy:\n",
    "            return self.typy_wczytane\n",
    "        return self.wczytywacz.types()\n",
    "    def engineer_feature(self,f,slownik,typ,nazwa):\n",
    "        assert self.not_compiled\n",
    "        self.wczytywacz.engineer_feature(f,slownik,typ,nazwa)\n",
    "        self.lista_transformacji.append({'f':f,'slownik':slownik,'typ':typ,'nazwa':nazwa})\n",
    "    def train(self,batch_size=128,buffer_size=1000,steps=3000):\n",
    "        self.classifier.train(\n",
    "        input_fn=lambda:self.input_fn( batch_size,buffer_size),\n",
    "        steps=steps)\n",
    "    def evaluate(self,batch_size=128,buffer_size=1000,steps=1000,folder=\"\"):\n",
    "        if folder==\"\":\n",
    "            folder=self.nazwa_folderu\n",
    "        self.last_eval_result = self.classifier.evaluate(\n",
    "        input_fn=lambda:self.input_fn( batch_size,buffer_size,folder=folder),\n",
    "        steps=steps)\n",
    "        return self.last_eval_result\n",
    "    \"\"\"\n",
    "    def input_fn_new_graph( folder,batch_size=100,buffer_size=1000,one_epoch=False,czy_shuffle=True\n",
    "                    ,czy_batch=True):\n",
    "        \n",
    "        g = tf.Graph()\n",
    "        with g.as_default():\n",
    "            dataset=io.Io_tf_binary_general(folder,'r').read()\n",
    "            if czy_shuffle:\n",
    "                dataset=dataset.shuffle(buffer_size)\n",
    "            if one_epoch:\n",
    "                dataset=dataset.repeat(1)\n",
    "            else:\n",
    "                dataset=dataset.repeat()\n",
    "            if czy_batch:\n",
    "                dataset=dataset.batch(batch_size)\n",
    "            return dataset\n",
    "            \"\"\"\n",
    "    def _labelki( folder,ile_take):\n",
    "        \"\"\"pomocnicza funkcja ktora zwraca generator labelkow przypadkow\"\"\"\n",
    "        #g = tf.Graph()\n",
    "        #with g.as_default():\n",
    "        dataset=io.Io_tf_binary_general(folder,'r').read().take(ile_take)\n",
    "\n",
    "        dataset=dataset.repeat(1)\n",
    "        dane=dataset\n",
    "        iterator = dane.make_one_shot_iterator()\n",
    "        para=iterator.get_next()\n",
    "        def wyrzut():\n",
    "            with tf.Session() as sess2:\n",
    "                while True:\n",
    "                    try:\n",
    "                        yield sess2.run(para)[1]\n",
    "                    except:\n",
    "                        break\n",
    "        return wyrzut()\n",
    "            \n",
    "            \n",
    "    \n",
    "    def evaluate_jak_z_pracy(self,p0to1,p1to1,ile_take=10000,folder=\"\"):\n",
    "        def generator_of_predictions_and_labels(self,batch_size=128,folder=\"\",ile_take=10000):\n",
    "            if folder==\"\":\n",
    "                folder=self.nazwa_folderu   \n",
    "            return (self.classifier.predict(input_fn=\n",
    "               lambda:self.input_fn(batch_size,folder=folder,one_epoch=True,czy_shuffle=False,take=True,ile_take=ile_take)),\n",
    "                    Dnn_uniwersalny._labelki(folder,ile_take)  )\n",
    "        #tu jest pewien problem. jak sie odpali ten generator rzeczy wynikajacych z predict to\\\n",
    "        #dopuki sie nie wykona nie da sie nic  robic na grafie. bo jest chwilowo finalized. wiec\n",
    "        #najpierw odczytam sobie jedne rzeczy a potem drugie. nie moge zrobic zip tych \n",
    "        # 2 generatorow. dlatego uzywam funkcji take ktora ograniczy nam to jak duzo danych dostaniemy\n",
    "        def ta_praw(a):\n",
    "            wyrzut=[]\n",
    "            for i in a:\n",
    "                wyrzut.append(i['probabilities'][1])\n",
    "            return wyrzut\n",
    "\n",
    "\n",
    "        def benchmark(gen_z_estimatora,gen_ground_truth_mixed_class,p0to1,p1to1):\n",
    "            \"\"\"p0to1 to prawdopodobienstow, ze jak cos ma labelke 0 to to jest 1\"\"\"\n",
    "            #przez wyniki rozumiem liczbe ktora im jest wieksza tym bardziej estymator mysli ze to jest 1\n",
    "            probabilities_of_being_1=ta_praw(gen_z_estimatora)\n",
    "            true_class_identity=list(gen_ground_truth_mixed_class)\n",
    "            #przez class rozumiem tu ta mieszanine z prawdopodpbienstwem\n",
    "\n",
    "            assert len(probabilities_of_being_1)==len(true_class_identity)\n",
    "\n",
    "            fpr, tpr, thresholds = metrics.roc_curve(true_class_identity\n",
    "                        , probabilities_of_being_1, pos_label=1)\n",
    "            eps_s_list=[]\n",
    "            eps_b_list=[]\n",
    "            for i in range(len(fpr)):\n",
    "                odw=np.linalg.inv(np.array([[p0to1,1-p0to1],[p1to1,1-p1to1]]))\n",
    "                epss_epsb=np.matmul(odw,np.array([[fpr[i]],[tpr[i]]]))\n",
    "                eps_s,eps_b=epss_epsb\n",
    "                eps_s_list.append(eps_s)\n",
    "                eps_b_list.append(eps_b)\n",
    "            return eps_s_list,eps_b_list,thresholds\n",
    "        a,b=generator_of_predictions_and_labels(self,batch_size=128,folder=folder,ile_take=10000)\n",
    "        eps_s,eps_b,thresholds=benchmark(a,b,p0to1,p1to1)\n",
    "        auc=metrics.auc(eps_b,eps_s,True)\n",
    "        print(\"auc wynosi \"+str(auc))\n",
    "        plt.clf()\n",
    "        plt.scatter(eps_b,eps_s)\n",
    "        plt.xlabel(\"eps_b\")\n",
    "        plt.ylabel(\"eps_s\")\n",
    "        return auc,eps_s,eps_b,thresholds\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ta linijka zaklada, ze zrobilismy plik dane_treningowe2 wedlug Io_tf_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pierwsza': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None), 'inna_wlasnosc': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None), 'kategoryczna': FixedLenFeature(shape=[1], dtype=tf.int64, default_value=None), 'label': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None)}\n"
     ]
    }
   ],
   "source": [
    "czytacz=io.Io_tf_binary_general(\"weak_validate\",'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "para=Dnn_uniwersalny.licz_normalizacje(czytacz,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-7.0203323], dtype=float32), array([68.99459], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(sess.run(tf.nn.moments(para[0]['inna_wlasnosc'],axes=[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'pierwsza': array([2.352334], dtype=float32), 'inna_wlasnosc': array([0.77512], dtype=float32)}, {'pierwsza': array([2.2443488], dtype=float32), 'inna_wlasnosc': array([2.1985173], dtype=float32)})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'pierwsza': <tf.Tensor 'moments_1/Squeeze:0' shape=(1,) dtype=float32>,\n",
       "  'inna_wlasnosc': <tf.Tensor 'moments_2/Squeeze:0' shape=(1,) dtype=float32>},\n",
       " {'pierwsza': <tf.Tensor 'pow:0' shape=(1,) dtype=float32>,\n",
       "  'inna_wlasnosc': <tf.Tensor 'pow_1:0' shape=(1,) dtype=float32>})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(sess.run(para))\n",
    "para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pierwsza': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None), 'inna_wlasnosc': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None), 'kategoryczna': FixedLenFeature(shape=[1], dtype=tf.int64, default_value=None), 'label': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None)}\n"
     ]
    }
   ],
   "source": [
    "model=Dnn_uniwersalny(\"weak_train\",[10],\"weak_estimator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'weak_estimator/tensorflowowy', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f6fe74e4f60>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "model.make_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.array([2])\n",
    "list(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pierwsza': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None), 'inna_wlasnosc': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None), 'kategoryczna': FixedLenFeature(shape=[1], dtype=tf.int64, default_value=None), 'label': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None)}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into weak_estimator/tensorflowowy/model.ckpt.\n",
      "INFO:tensorflow:loss = 97.53022, step = 1\n",
      "INFO:tensorflow:global_step/sec: 83.4305\n",
      "INFO:tensorflow:loss = 79.86455, step = 101 (1.200 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.0761\n",
      "INFO:tensorflow:loss = 81.04253, step = 201 (1.135 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.5387\n",
      "INFO:tensorflow:loss = 80.03941, step = 301 (1.155 sec)\n",
      "INFO:tensorflow:global_step/sec: 89.0413\n",
      "INFO:tensorflow:loss = 83.0307, step = 401 (1.123 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.3676\n",
      "INFO:tensorflow:loss = 88.55333, step = 501 (1.158 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.7615\n",
      "INFO:tensorflow:loss = 79.20351, step = 601 (1.139 sec)\n",
      "INFO:tensorflow:global_step/sec: 92.6441\n",
      "INFO:tensorflow:loss = 76.97481, step = 701 (1.080 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.5815\n",
      "INFO:tensorflow:loss = 70.64668, step = 801 (1.182 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.3597\n",
      "INFO:tensorflow:loss = 76.58958, step = 901 (1.199 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.8834\n",
      "INFO:tensorflow:loss = 79.25853, step = 1001 (1.138 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.7724\n",
      "INFO:tensorflow:loss = 79.10007, step = 1101 (1.139 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.6284\n",
      "INFO:tensorflow:loss = 72.63895, step = 1201 (1.141 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.0631\n",
      "INFO:tensorflow:loss = 74.14648, step = 1301 (1.136 sec)\n",
      "INFO:tensorflow:global_step/sec: 91.0051\n",
      "INFO:tensorflow:loss = 79.24994, step = 1401 (1.099 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.6191\n",
      "INFO:tensorflow:loss = 82.086296, step = 1501 (1.141 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.3856\n",
      "INFO:tensorflow:loss = 79.0919, step = 1601 (1.132 sec)\n",
      "INFO:tensorflow:global_step/sec: 89.3166\n",
      "INFO:tensorflow:loss = 74.83101, step = 1701 (1.119 sec)\n",
      "INFO:tensorflow:global_step/sec: 89.9321\n",
      "INFO:tensorflow:loss = 92.27698, step = 1801 (1.112 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.9624\n",
      "INFO:tensorflow:loss = 74.95406, step = 1901 (1.137 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.0158\n",
      "INFO:tensorflow:loss = 75.61609, step = 2001 (1.163 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.6912\n",
      "INFO:tensorflow:loss = 74.54036, step = 2101 (1.140 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.0397\n",
      "INFO:tensorflow:loss = 71.502106, step = 2201 (1.149 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.1857\n",
      "INFO:tensorflow:loss = 77.85536, step = 2301 (1.147 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.3303\n",
      "INFO:tensorflow:loss = 75.90849, step = 2401 (1.132 sec)\n",
      "INFO:tensorflow:global_step/sec: 89.2965\n",
      "INFO:tensorflow:loss = 85.94804, step = 2501 (1.120 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.4353\n",
      "INFO:tensorflow:loss = 80.08791, step = 2601 (1.144 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.4543\n",
      "INFO:tensorflow:loss = 83.638405, step = 2701 (1.170 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.6788\n",
      "INFO:tensorflow:loss = 85.44583, step = 2801 (1.128 sec)\n",
      "INFO:tensorflow:global_step/sec: 89.4303\n",
      "INFO:tensorflow:loss = 74.56908, step = 2901 (1.118 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3000 into weak_estimator/tensorflowowy/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 74.95264.\n"
     ]
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pierwsza': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None), 'inna_wlasnosc': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None), 'kategoryczna': FixedLenFeature(shape=[1], dtype=tf.int64, default_value=None), 'label': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None)}\n",
      "{'pierwsza': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None), 'inna_wlasnosc': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None), 'kategoryczna': FixedLenFeature(shape=[1], dtype=tf.int64, default_value=None), 'label': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None)}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from weak_estimator/model.ckpt-3000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "auc wynosi 0.9863495170618184\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9863495170618184"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAELCAYAAAA2mZrgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAH4dJREFUeJzt3X+QVeWd5/H3ty+X2BhjQ2irYkuLMYgDopLpFS22duKaHfwxgR4HfzC6O8m6Wjs7JuOY7S2TWAqGGd1QiRVn3E2cGctkNIoypgMJKVKVkHXGDQrYINNGKmgQaDILiUASaLVpvvvH/cHt2+eee7q5597ufj6vKst77zmc+xzoPp/z/DjPY+6OiIgIQFOjCyAiImOHQkFERIoUCiIiUqRQEBGRIoWCiIgUKRRERKRIoSAiIkUKBRERKVIoiIhI0aRGF2Ckpk+f7jNnzmx0MURExpWtW7f+0t1bq+037kJh5syZbNmypdHFEBEZV8zsrST7qflIRESKFAoiIlKkUBARkSKFgoiIFCkURESkSKEgIiJFCgURESlSKIiISNG4e3hNRIbq7ulj1Yad7D/cz9ktzXQtmk3n/LZGF0vGKYWCyDh1b/cOntq0By/5rO9wP11rtgMkCobRBkrpnzuzOYsZHD42UDwGoKAap8zdq+81hnR0dLimuZBQFC6+fYf7yZgxmPD3deqULD33/X7sPrf87U948Y23h3yWzRirll5SvICXfn9STQZmxuCJk2UtP27BSEOpPIzeOz7IsYETABgUA7KlOcsfXPIhNr5+cNixQ61ZmdlWd++oul9aoWBmjwN/ABxw94sithvwVeBa4BjwSXd/pdpxFQoy0Y3mQhxl90PXVdx2b/cOnty0J3JbIVC6e/roWrOdgcHaXCPKgyrq+FHhUfr3UXrhH6lsxrjp38xg9ea9Vb8zyqmEyVgIoqShkGbz0RPA3wDfrLD9GmBW/r8FwP/O/19kTOru6eNzz79Kf/7OtMngjxe0A/D0S3sZdCdjxrIFM1jZOW9UF4Kou/c0PP3S3orbDh0bAGDFut6aBULpcQuijj8w6KxY1zukplIaHKdSmoFBH9bcFvWd5So10/3F6m1seSv3b1UesHHhFdfEV61Zrh5BkloouPsLZjYzZpclwDc9V1XZZGYtZvYhd/9FWmUSGYnSX9BJTZDPgqITPvxiMOjOk5v28PODv+WVPUfoHxgEhl5EVnbOi/y+WgZCS3M2dnuSZqjyi3itVTp+6ee1DqZKR6pUlrgalTP837/a9xREBVF5AB7uP1mmkfYVnYpGdjS3AaW3K/vynykUpKLunj6Wr+0t/sJMnZLl/k/MrekvSndPH13PbRsSAuWBUE3Uxd2BpzbtoePcacMuBp9//tVi2/ipyjYZyxfPjd0nrn+iWqCM1miOm3YwVRNXozpVSWpOparVaGqlkaFgEZ9F/o2Y2R3AHQDt7e1plkkapFLTTOldddSd9KFjA9xVdgd+b/eOyOacpOW4e/U2anN5Hs7Jjcqp1Dxyqk6fnOEv/3Be1QvHsgUzIu9yDYqB0tKcHXK3mlRT/kAl/cyRQVXp+GmFUpxK35m0Y78WkgRgPUKykaGwD5hR8v4cYH/Uju7+GPAY5Dqa0y9amCq1gZdeZEs1Z5s4LZvh8LGBIe2f1dpCyy/us846nTcOHB1yIS5tmlnZOY97u3fENq0U7sC3vPX2kItdoTmncJzy8y0PouZsJrVAKNhf0oG8asPOmgTCSMOvsF9pW3l5oCxfPJeu57YzcCK+fJMzxunvmzTiIalRxy8Pj9EGU3O2iXePnxgWTDddNoPVL++N/c5SIxnxNVKNCL8kGhkKa4E7zewZch3MR0LsT0hjVMJohvmVNsnAyTbM57bsqXgx7h84Ubyglv7ZuLbQqLv9nx04WrFsT7+0l5Wd8/jWS9FttwWFO/B/PfJO7HEKunv6uGv1tiH7nHA4+t5g7PcktfD8afzfN96OrPqe3dJcfL1/FCOM2mr0c7Kyc15siBSOXz4kNun3J90e97OaNJhgeFNipd+DjnOnJf79qFSjKmiCUd1EjKTmVL5P2lILBTN7GvgYMN3M9gH3A1kAd/8asJ7ccNRd5IakfiqtsowFURdegEzTyfHcfYf7+dzzOwCK46lL72SLfyZ/VwhDO7oMOOuMyfy/37xX/Cyqg6rSnX+5gUGvScdnaVvoSI9XKGOCawL7D/dX7OArP9cV63pHVI5KFp4/jfNa3x/ZXBU1aqU5myneSUMuIKoNPU3aJJSGzvltqX5vteNHBceVF7ZGPn+Q9NgjOadCaBb+fQ2YMjnDsfcGi99dXjuFoaOPykciVQrVagGYpK+oFvTwWoRKdxhRDxKV3j2V/7BeeWEr393+ixFXfws/NOV3sqeiMEY8bjRF2nY/dB0z7/lean+uraWZfz3yTmTYZcx448Fri+9HU45SSS/U1Wpt1foUbr28PXGTkIx/aQ5JHQvPKYwbcQ8LFYYSPrdlDy/vPlT85S1ceAr/7zvcP+RiW/5+JPoO99fsTrag0EGV5miKNDVnm4bVmIZuz1S8awOKNatTYcDDN106ol/MpHfCK9b1DulETGNUlYx9adfMkgg+FJKM/nCihximJWOW2iiDeo6mKFVoC81GjPeP05Zvf3/w+osrjgoqrY4XfqGqjT4aTQfmSAMhqbFwIRApCD4Uav1wTC2kceEuXJRHMpoi22Rcdt7UUw7E0rbQVTdcGtksduvl7fzj1r7iw14wtP09SadkQbUOVKjcfjvrrNPZ/ctjwz6/9fJ2XbglCMGHQqMfjonS1tLM0XePj2ooXpTSsefVRlMUlDZfVBr3X6n9M64tNO7iXm1USC3vqOPKMRbmqRFplOA7mk+1w/FUNZU95NOczfDg9bm73EojEeJGH33krNPZdeBoxbHnED/6qJEjXUQkPepoTqC7py/xvtmmXLNLkqGRSRQuvhDfJFLtjnU0I1OSNK+ISJiCril85PPrOV7lKm8w7CnNSkNSy5/kLDSjHDo2MOIHf0REakk1hSru7d5RNRDaWpp58Z5/P+SzJBdzXfBFZLxqanQBGuWpKtMmAEOePBURCUGwoVCt1UxDEEUkRMGGQpxs0+g6cEVExrtgQ2FKtvKpr7rh0jqWRERk7Ag2FP7q+otpiljmR81GIhKyYEcflc8VX1CYME7NRyISomBrCpALhisvbB3yWWGlrnu7dzSoVCIijRN0KAAVV/Qar1NMi4iciqBDobunr+K0FY2aYlpEpJGCDoVaL2QjIjLeBR0KY3HabBGRRgo6FEREZKigQ2FyJuJBhQTbREQmqqBDIW4ZzrG2RKeISD0EHQpxl31FgoiEKOhQyFjlJqK4bSIiE1XQoVBY5zjK5R+eWseSiIiMDUGHwsrOeSw8f1rktpd3HxrRGs4iIhNB0KEAsPtX/ZGfDww6qzbsrHNpREQaK/hQ2H84OhSqbRMRmYiCD4WzW5pHtU1EZCJKNRTM7Goz22lmu8zsnojt7Wa20cx6zOxVM7s2zfJE6Vo0m2zEajvZjNG1aHa9iyMi0lCphYKZZYBHgWuAOcAyM5tTttu9wLPuPh+4GfhfaZWnks75bay64RJamrPFz6ZOybJq6SVagU1EgpPmymuXAbvc/U0AM3sGWAK8VrKPAx/Ivz4T2J9ieSrqnN+mABARId1QaANKV6rZBywo22c58AMz+zRwOvDxFMsjIiJVpBkKUY8El88esQx4wt2/bGZXAP9gZhe5+4khBzK7A7gDoL29PZXCFnT39LF8bS+H+3PTak+dkuX+T8xVTUJEgpBmR/M+oPSR4XMY3jx0G/AsgLv/BDgNmF5+IHd/zN073L2jtbW1fHPNdPf00fXc9mIgQG7Nha412/Ugm4gEIc1Q2AzMMrPzzGwyuY7ktWX77AGuAjCz3yEXCgdTLFOsFet6GYhYn1MPsolIKFILBXc/DtwJbAB+Sm6UUa+ZPWBmi/O7fRa43cy2A08Dn3RvzOLI3T19sSux9elBNhEJQJp9Crj7emB92Wf3lbx+DViYZhmSqlYT0KypIhKC4J9oLqg2pcVgYyowIiJ1pVDIqzalRZumvBCRACgU8roWzaY5m4ncpikvRCQUCoW8zvlt/NHvRj+LMBgxIklEZCJSKJT43qu/iPz8hFfviBYRmQgUCnkakioiolAoWrGuN3a7hqSKSAgUClSvJYCGpIpIGBQKJOsv0JBUEQmBQoFk/QUakioiIVAoABGrcQ7R0pzV1NkiEgSFArkhp3GWL55bn4KIiDSYQiEB1RJEJBQKBaKXiEuyTURkolEoMHyN0KTbREQmGoVCFXpoTURCEnwo/Iev/Dh2ux5aE5GQBB0K3T19/OzA0dh9VFMQkZAEHQrV5jsC1RREJCxBh0K1+Y5A01uISFiCDoVqmrMZTW8hIkEJOhRamrOx2x+8fp4eXBORoAQdCnPPPqPitvdNalIgiEhwgg2F7p4+Xnzj7Yrb3z1+oo6lEREZG4INBa25LCIyXLChoDWXRUSGCzYUqj2UtvD8aXUqiYjI2BFsKMQ9lNYEPHX7FfUrjIjIGBFsKMQ9lKYuZhEJVaqhYGZXm9lOM9tlZvdU2OdGM3vNzHrN7FtplqeUHkoTERkutVAwswzwKHANMAdYZmZzyvaZBXwOWOjuc4G70ipPOT2DICIyXJo1hcuAXe7+pru/BzwDLCnb53bgUXc/BODuB1IszzCVOps1M6qIhCrNUGgD9pa835f/rNQFwAVm9qKZbTKzq1MszzDLFswY0eciIhPdpBSPHXW7XT7kZxIwC/gYcA7wT2Z2kbsfHnIgszuAOwDa29trVsCVnfMAePqlvQy6kzFj2YIZxc9FREKTZk1hH1B6y30OsD9in++4+4C7/xzYSS4khnD3x9y9w907Wltba1rIlZ3z+PKNl9DSnGXQnSc37WH+Az+gu6evpt8jIjIepBkKm4FZZnaemU0GbgbWlu3TDVwJYGbTyTUnvZlimYa5t3sHd63exuH+k2srHDo2QNea7QoGEQlOaqHg7seBO4ENwE+BZ92918weMLPF+d02AL8ys9eAjUCXu/8qrTKV6+7p48lNeyK3DQy65kcSkeAk6lMws9OBfnc/YWYXABcC33f32KXL3H09sL7ss/tKXjtwd/6/uqt20d+v+ZFEJDBJawovAKeZWRvwQ+BTwBNpFapeqk2Kd7aW4hSRwCQNBXP3Y8D1wF+7+x+SeyBt3ErSX6CnnkUkNIlDwcyuAG4Bvpf/LM3hrKlbsa636j566llEQpM0FP6c3HQU3853Fn+YXMfwuHXoWGx3iIhIkBLd7bv7C+T6FQrv3wQ+U3hvZn/t7p+uffEap6U52+giiIjUXa2GpC6s0XHqJlNleqPli+fWpyAiImNIkOspdPf0MVh5jR2yTepPEJEwBRkK1TqZB7TKjogEqlahMG7mmu7u6VMns4hIBSMOBTNrMrMPlH381RqVJ3VJpq5QJ7OIhCpRKJjZt8zsA/npLl4DdppZV2G7uz+RUvlqrtrUFU2mTmYRCVfSmsIcd/810EluLqN24D+mVqoUxU1dYcBXbrxUncwiEqykoZA1syy5UPhOfiK8mPE7Y1fXotlkm6K7QMblCYmI1FDSUPg6sBs4HXjBzM4Ffp1WodLUOb+Nmy6rvNzm559/tY6lEREZW5I+0fwI8EjJR2+Z2ZXpFCl9G18/WHHbMY1HFZGAJe1o/qCZPWJmr5jZVjP7KnBmymVLjdZJEBGJlrT56BngIPBHwNL869VpFSptsZ3N4+aJCxGR2ksaCtPc/Yvu/vP8fyuBljQLlqauRbMrnvgtC9rrWhYRkbEkaShsNLOb8w+uNZnZjZxcV2Hc6ZzfxlduupTm7MnTbzK49fJ2VnbOa2DJREQay3LLJFfZyew3wBSg0AubAY7mX7u7lz/hnJqOjg7fsmVLzY7X3dPH8rW9HO7PTX0xdUqW+z8xV88qiMiEYmZb3b2j2n5JV087k9yqa+e5+wNm1g58yN1fOpVCNlp3Tx9dz21n4MTJYDx0bICuNdsBzZQqIuFJ2nz0KHA5sCz//jfA36RSojpatWHnkEAoGBj0RHMkiYhMNElrCgvc/aNm1gPg7ofMbHKK5aqLvpihqXHbREQmqqQ1hQEzy5CfCcLMWjnZvzBuZWLGn8ZtExGZqJKGwiPAt4GzzOwvgX8G/iq1UtXJYEwne9w2EZGJKuk0F0+Z2VbgKnKTiXa6+09TLVkdZMwqXvynTtGaCiISnqR9Crj768DrKZal7uJqA6ooiEiIglyjuaAtZrqLI/1aslNEwhN0KHQtml1xcem4+ZFERCaqVEPBzK42s51mtsvM7onZb6mZuZlVfdquljrnt3HL5dFzHV15YWs9iyIiMiakFgr5IayPAtcAc4BlZjYnYr8zgM8ADXk6uuPcaZF/Cas376W7p6/u5RERaaQ0awqXAbvc/U13f4/c9NtLIvb7IvAl4J0Uy1LRqg07Ix+40FPNIhKiNEOhDdhb8n5f/rMiM5sPzHD378YdyMzuMLMtZrbl4MHKq6aNRtyCO1qMR0RCk2YoRPXhFgd6mlkT8DDw2WoHcvfH3L3D3TtaW2vb1h/XoazOZhEJTZqhsA+YUfL+HGB/yfszgIuAH5vZbnIT7q2td2dz16LZZJuG51c2Y3Qtml3PooiINFyaobAZmGVm5+Unz7sZWFvY6O5H3H26u89095nAJmCxu9dusYQEOue3seqGS2hpPvkE89QpWVYtvURTZ4tIcFILBXc/DtwJbAB+Cjzr7r1m9oCZLU7re0ejc34byxfPLT7M9uv+49y1ehsLH/qRRiCJSFAST3MxGu6+Hlhf9tl9Ffb9WJplidPd00fXmu0MDOa6PArTX/Qd7udzz+8AtOCOiIQh6CeaC1as6y0GQrn+gUENTRWRYCgUyC3BGUdDU0UkFAqFBDQ0VURCoVAg+oGKUhqaKiKhUChQ8kRdBepkFpFQKBSIX2VNK7CJSEiCD4Xunr7YBXV++44W2xGRcAQfCivW9XIipv1oIGoKVRGRCSr4UKg2HFVEJCTBh0I1Vm1okojIBKJQqOKWBdHLdYqITEQKhRgLz5/Gys55jS6GiEjdKBRiPHX7FY0ugohIXQUdCpoWW0RkqKBDQbOfiogMFXQoaPZTEZGhgg6FuNlP2zQzqogEKOhQ6Fo0m2zT8AcRshnTzKgiEqRUl+Mc6wqzny5f28vh/PxHU6dkuf8TczUzqogEKehQgFwwKABERHKCbj4SEZGhgq8plOru6VNTkogETaGQ193Tx93PbhsyjfahYwN89rntgFZfE5EwqPkor9K6CoMnnBXreutfIBGRBlAo5MWtq6A1F0QkFAoFNAeSiEiBQgGqNg9NyeqvSUTCEPzVrrunr2rz0ORJmTqVRkSksVIdfWRmVwNfBTLA37n7Q2Xb7wb+C3AcOAj8Z3d/K80yFZQPP41zJME+IiITQWo1BTPLAI8C1wBzgGVmNqdstx6gw90vBtYAX0qrPKXu7d7BXau3JQoEiJ84T0RkIkmz+egyYJe7v+nu7wHPAEtKd3D3je5+LP92E3BOiuUBcjWEJzftGdGf0eR4IhKKNEOhDdhb8n5f/rNKbgO+n2J5gNEtrKMH10QkFGn2KQyfkxoiHg8DM7sV6AB+r8L2O4A7ANrb20+pUCNdWEfrKohISNKsKewDZpS8PwfYX76TmX0c+AKw2N3fjTqQuz/m7h3u3tHa2npKhWqZkk28b3M2o6YjEQlKmqGwGZhlZueZ2WTgZmBt6Q5mNh/4OrlAOJBiWYo8sq4yXFtLMw9eP09NRyISlNSaj9z9uJndCWwgNyT1cXfvNbMHgC3uvhZYBbwfeM7MAPa4++K0ygQkGnG0+6Hr0iyCiMiYlepzCu6+Hlhf9tl9Ja8/nub3R8mYMRhTXVAfgoiELLgnmuMCwdDwUxEJW3ChEFcTcDT8VETCFlwodC2aHTlWFnIrrYmIhCy4UOic38ZHzjo9ctuR/gFNoy0iQQsuFLp7+vjZgaOR20746J54FhGZKIILhWprJ4z0iWcRkYkkuFCotnaCZkQVkZAFFwrVaEiqiIQsuFDIVBp6lKchqSISsqBCobunj8GEcx+JiIQoqFDQyCIRkXhBhUJflZFFGavStiQiMsEFFQrVLvrLFsyI3S4iMtEFFQpxk+EBrOycV6eSiIiMTUGFgpqHRETiBRUK1WoKIiKhCyoUVFMQEYkXVCiopiAiEi+oUIhbL0FrKYiIBBYKcRUFVSJERAILhSP9lWdIjdsmIhKKoEIhblpsTZktIhJYKHQtmk22afgIpGzGNGW2iAiBhULn/DZW3XAJLc1DO5UHBp0V63q1PrOIBG9SowtQb4X1Eu5+dhsnSjqXDx0boGvN9iH7iIiEJqiaQsEXvr1jSCAUDAy6ptcWkaAFFwrdPX0cfW+w4vZq02uLiExkwYVCtZqApsIQkZAFFwr7q9QENBWGiIQs1VAws6vNbKeZ7TKzeyK2v8/MVue3v2RmM9MsD1R/HqFNzyuISMBSCwUzywCPAtcAc4BlZjanbLfbgEPu/hHgYeB/plWegkrPKhRceWFr2kUQERmz0qwpXAbscvc33f094BlgSdk+S4Bv5F+vAa4yS7dRv/CsQqUv2fj6wTS/XkRkTEszFNqAvSXv9+U/i9zH3Y8DR4APplgmIP45hGp9DiIiE1maoRB1M17ei5tkH8zsDjPbYmZbDh6szZ18pb4FzYEkIiFLMxT2ATNK3p8D7K+0j5lNAs4E3i4/kLs/5u4d7t7R2lqbNv+uRbNpzmaGfNaczWgOJBEJWpqhsBmYZWbnmdlk4GZgbdk+a4E/yb9eCvzIvT5jQjvnt/Hg9fNoa2nGyI06evD6eZriQkSCltrcR+5+3MzuBDYAGeBxd+81sweALe6+Fvh74B/MbBe5GsLNaZUnSuf8NoWAiEiJVCfEc/f1wPqyz+4ref0OcEOaZRARkeSCe6JZREQqUyiIiEiRQkFERIoUCiIiUqRQEBGRIoWCiIgUKRRERKTI6vQAcc2Y2UHgrRofdjrwyxofc6zSuU5cIZ2vznXkznX3qvMEjbtQSIOZbXH3jkaXox50rhNXSOerc02Pmo9ERKRIoSAiIkUKhZzHGl2AOtK5Tlwhna/ONSXqUxARkSLVFEREpCioUDCzq81sp5ntMrN7Ira/z8xW57e/ZGYz61/K2khwrneb2Wtm9qqZ/dDMzm1EOWuh2rmW7LfUzNzMxu2olSTnamY35v9te83sW/UuY60k+BluN7ONZtaT/zm+thHlrAUze9zMDpjZv1TYbmb2SP7v4lUz+2hqhXH3IP4jt9DPG8CHgcnAdmBO2T7/Dfha/vXNwOpGlzvFc70SmJJ//acT+Vzz+50BvABsAjoaXe4U/11nAT3A1Pz7sxpd7hTP9THgT/Ov5wC7G13uUzjffwd8FPiXCtuvBb5Pbl37y4GX0ipLSDWFy4Bd7v6mu78HPAMsKdtnCfCN/Os1wFVmZnUsY61UPVd33+jux/JvN5FbQ3s8SvLvCvBF4EvAO/UsXI0lOdfbgUfd/RCAux+ocxlrJcm5OvCB/OszGb4G/Ljh7i8QsT59iSXANz1nE9BiZh9KoywhhUIbsLfk/b78Z5H7uPtx4AjwwbqUrraSnGup28jdhYxHVc/VzOYDM9z9u/UsWAqS/LteAFxgZi+a2SYzu7pupautJOe6HLjVzPaRW+Hx0/UpWkOM9Hd61FJdjnOMibrjLx96lWSf8SDxeZjZrUAH8Huplig9sedqZk3Aw8An61WgFCX5d51ErgnpY+Rqf/9kZhe5++GUy1ZrSc51GfCEu3/ZzK4gt977Re5+Iv3i1V3drk0h1RT2ATNK3p/D8OpmcR8zm0SuShpXpRurkpwrZvZx4AvAYnd/t05lq7Vq53oGcBHwYzPbTa49du047WxO+jP8HXcfcPefAzvJhcR4k+RcbwOeBXD3nwCnkZsnaCJK9DtdCyGFwmZglpmdZ2aTyXUkry3bZy3wJ/nXS4Efeb6XZ5ypeq75JpWvkwuE8druDFXO1d2PuPt0d5/p7jPJ9Z8sdvctjSnuKUnyM9xNbhABZjadXHPSm3UtZW0kOdc9wFUAZvY75ELhYF1LWT9rgf+UH4V0OXDE3X+RxhcF03zk7sfN7E5gA7mRDY+7e6+ZPQBscfe1wN+Tq4LuIldDuLlxJR69hOe6Cng/8Fy+L32Puy9uWKFHKeG5TggJz3UD8Ptm9howCHS5+68aV+rRSXiunwX+1sz+glxTyifH6U0cZvY0uSa/6fk+kvuBLIC7f41cn8m1wC7gGPCp1MoyTv8ORUQkBSE1H4mISBUKBRERKVIoiIhIkUJBRESKFAoiIlKkUBARkSKFgkgdmdlyM/vvjS6HSCUKBRERKVIoiFRgZrea2ctmts3Mvm5mGTP7rZl92cxeyS9O1Jrf9zMlixY9U+XQl5jZj8zsZ2Z2ex1ORSQxhYJIhPxcOjcBC939UnJTRtwCnA684u4fBf4PuekIAO4B5rv7xcB/rXL4i4HrgCuA+8zs7BROQWRUFAoi0a4CfhfYbGbb8u8/DJwAVuf3eRL4t/nXrwJP5aciP17l2N9x9353/yWwkdyCMiJjgkJBJJoB33D3S/P/zXb35RH7FSYPuw54lFyQbM1PvV5J+YRjmoBMxgyFgki0HwJLzewsADObZmbnkvudWZrf54+Bf84v5DPD3TcC/wNoITcDbSVLzOw0M/sguZkxN6d0DiIjFszU2SIj4e6vmdm9wA/yF/0B4M+Ao8BcM9tKbrnWm8hN7fykmZ1JrobxcJWVzl4Gvge0A19093G7trBMPJo6W2QEzOy37h5XCxAZ19R8JCIiRaopiKTAzD4F/HnZxy+6+581ojwiSSkURESkSM1HIiJSpFAQEZEihYKIiBQpFEREpEihICIiRf8f4jO7RM0KJzIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a=model.evaluate_jak_z_pracy(0.2,0.8,folder=\"weak_validate\")\n",
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pierwsza': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None), 'inna_wlasnosc': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None), 'kategoryczna': FixedLenFeature(shape=[1], dtype=tf.int64, default_value=None), 'label': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None)}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-05-21-22:34:26\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from weak_estimator/model.ckpt-3000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [100/1000]\n",
      "INFO:tensorflow:Evaluation [200/1000]\n",
      "INFO:tensorflow:Evaluation [300/1000]\n",
      "INFO:tensorflow:Evaluation [400/1000]\n",
      "INFO:tensorflow:Evaluation [500/1000]\n",
      "INFO:tensorflow:Evaluation [600/1000]\n",
      "INFO:tensorflow:Evaluation [700/1000]\n",
      "INFO:tensorflow:Evaluation [800/1000]\n",
      "INFO:tensorflow:Evaluation [900/1000]\n",
      "INFO:tensorflow:Evaluation [1000/1000]\n",
      "INFO:tensorflow:Finished evaluation at 2018-05-21-22:34:35\n",
      "INFO:tensorflow:Saving dict for global step 3000: accuracy = 1.0, accuracy_baseline = 0.502, auc = 1.0, auc_precision_recall = 1.0, average_loss = 0.37425232, global_step = 3000, label/mean = 0.498, loss = 47.904297, precision = 1.0, prediction/mean = 0.50209004, recall = 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 1.0,\n",
       " 'accuracy_baseline': 0.502,\n",
       " 'auc': 1.0,\n",
       " 'auc_precision_recall': 1.0,\n",
       " 'average_loss': 0.37425232,\n",
       " 'label/mean': 0.498,\n",
       " 'loss': 47.904297,\n",
       " 'precision': 1.0,\n",
       " 'prediction/mean': 0.50209004,\n",
       " 'recall': 1.0,\n",
       " 'global_step': 3000}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(folder=\"weak_true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
